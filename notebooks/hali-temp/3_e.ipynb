{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ZDkiOQJ5F2Hw"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from bpemb import BPEmb\n",
        "import torch "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bpemb\n"
      ],
      "metadata": {
        "id": "L2JTQorBHzXS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhAAazmUGTLO",
        "outputId": "71a97e1b-8b4b-4a0b-ff20-e50b9fe6069b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        " device = torch.device(\"cuda\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dZlbMs0Ywew",
        "outputId": "158910be-37d7-4418-d955-1a86c57aa409"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import training data\n",
        "dft_eng = pd.read_csv('dft_eng.csv')\n",
        "\n",
        "# import validation data\n",
        "dfv_eng = pd.read_csv('dfv_eng.csv')\n",
        "\n",
        "embedding = torch.load('last_hidden.pt', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "RkJM8j6QGY9t"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_en = BPEmb(lang='en', dim=100, vs=25000)\n",
        "bpemb_en.vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erv3V_0yHaSQ",
        "outputId": "1ae09598-a67b-42b2-d50a-663a513f21e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.305192, -0.486759, -0.361542, ..., -0.205049,  0.33516 ,\n",
              "        -0.428452],\n",
              "       [-0.015292, -0.072622,  0.131374, ..., -0.291093, -0.15548 ,\n",
              "        -0.329501],\n",
              "       [ 0.266255,  0.113249, -0.081075, ..., -0.316839,  0.012411,\n",
              "        -0.232759],\n",
              "       ...,\n",
              "       [ 0.608477, -0.223953, -1.449336, ..., -0.07385 , -0.800959,\n",
              "         0.657389],\n",
              "       [ 0.167647, -0.133789,  0.252258, ...,  0.708369, -0.09607 ,\n",
              "         0.120539],\n",
              "       [-0.039888, -0.158139,  0.41632 , ..., -0.181057,  0.534418,\n",
              "        -0.487808]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1jbCXj8IDPr",
        "outputId": "6b8b23d5-0b56-4964-ed94-9f91cb2acf2b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "From https://github.com/hrobjarturh/nlptasks\n",
            "   8eedb6a..db99f32  main       -> origin/main\n",
            "Updating 8eedb6a..db99f32\n",
            "Fast-forward\n",
            " data/last_hidden.pt        | Bin \u001b[31m0\u001b[m -> \u001b[32m3819\u001b[m bytes\n",
            " data/last_hidden_trans.txt | 155 \u001b[31m---------------------------------------------\u001b[m\n",
            " 2 files changed, 155 deletions(-)\n",
            " create mode 100644 data/last_hidden.pt\n",
            " delete mode 100644 data/last_hidden_trans.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJzlgkvgWRKO",
        "outputId": "76945ed9-0972-4f90-a90b-a4dfa76c344b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mnotebooks/hali-temp/colab_subtask2-2.ipynb\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_LM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM Language Model\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,            \n",
        "            pretrained_embeddings: torch.tensor,\n",
        "            lstm_dim: int,       \n",
        "            dropout_prob: float = 0.0,\n",
        "            lstm_layers: int = 1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializer for LSTM Language Model\n",
        "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
        "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
        "        :param dropout_prob: Dropout probability\n",
        "        :param lstm_layers: The number of stacked LSTM layers\n",
        "        \"\"\"\n",
        "\n",
        "        # First thing is to call the superclass initializer\n",
        "        super(LSTM_LM, self).__init__()\n",
        "\n",
        "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
        "        # The components are an embedding layer, an LSTM layer, a dropout layer, and a feed-forward output layer\n",
        "        self.vocab_size = pretrained_embeddings.shape[0]\n",
        "        self.model = nn.ModuleDict({\n",
        "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
        "            'lstm': nn.LSTM( \n",
        "                pretrained_embeddings.shape[1],\n",
        "                lstm_dim,\n",
        "                num_layers=lstm_layers,\n",
        "                batch_first=True,\n",
        "                dropout=dropout_prob),\n",
        "            'ff': nn.Linear(lstm_dim, pretrained_embeddings.shape[0]),\n",
        "            'drop': nn.Dropout(dropout_prob)\n",
        "        })\n",
        "\n",
        "        # Initialize the weights of the model\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        all_params = list(self.model['lstm'].named_parameters()) + \\\n",
        "                     list(self.model['ff'].named_parameters())\n",
        "        for n, p in all_params:\n",
        "            if 'weight' in n:\n",
        "                nn.init.xavier_normal_(p)\n",
        "            elif 'bias' in n:\n",
        "                nn.init.zeros_(p)\n",
        "\n",
        "    def forward(self, input_ids, input_lens, hidden_states):\n",
        "        \"\"\"\n",
        "        Defines how tensors flow through the model\n",
        "        :param input_ids: (b x sl) The IDs into the vocabulary of the input samples\n",
        "        :param input_lens: (b x 1) The length of each instance's text\n",
        "        :param hidden_states: (b x sl) x 2 Hidden states for the LSTM model\n",
        "        :return: (lstm output, updated hidden stated)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings (b x sl x edim)\n",
        "        embeds = self.model['drop'](self.model['embeddings'](input_ids))\n",
        "\n",
        "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
        "            embeds,\n",
        "            input_lens.to('cpu'),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Pass the packed sequence through the BiLSTM\n",
        "        lstm_out, hidden = self.model['lstm'](lstm_in)\n",
        "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
        "        lstm_out, hidden_states = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        lstm_out = self.model['drop'](lstm_out)\n",
        "        # generate the prediction of each word in the vocabulary being the next\n",
        "        lstm_out = self.model['ff'](lstm_out)\n",
        "        lstm_out = lstm_out.reshape(-1, self.vocab_size)\n",
        "\n",
        "        return lstm_out, hidden_states"
      ],
      "metadata": {
        "id": "IdYKqfybTxLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}