{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from bpemb import BPEmb\n",
    "from tqdm import tqdm\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>question_text_tokenized</th>\n",
       "      <th>document_plaintext_tokenized</th>\n",
       "      <th>label</th>\n",
       "      <th>word_frequency_score</th>\n",
       "      <th>logres_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([159]), 'answer_text': ...</td>\n",
       "      <td>Quantum field theory naturally began with the ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "      <td>['When', 'was', 'quantum', 'field', 'theory', ...</td>\n",
       "      <td>['Quantum', 'field', 'theory', 'naturally', 'b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first Nobel prize winner for Liter...</td>\n",
       "      <td>List of Nobel laureates in Literature</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([610]), 'answer_text': ...</td>\n",
       "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Nobe...</td>\n",
       "      <td>['Who', 'was', 'the', 'first', 'Nobel', 'prize...</td>\n",
       "      <td>['The', 'Nobel', 'Prize', 'in', 'Literature', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When is the dialectical method used?</td>\n",
       "      <td>Dialectic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([129]), 'answer_text': ...</td>\n",
       "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dialectic</td>\n",
       "      <td>['When', 'is', 'the', 'dialectical', 'method',...</td>\n",
       "      <td>['Dialectic', 'or', 'dialectics', '(', 'Greek'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who invented Hangul?</td>\n",
       "      <td>Origin of Hangul</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([88]), 'answer_text': a...</td>\n",
       "      <td>Hangul was personally created and promulgated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Origin%20of%20Ha...</td>\n",
       "      <td>['Who', 'invented', 'Hangul', '?']</td>\n",
       "      <td>['Hangul', 'was', 'personally', 'created', 'an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do Grasshoppers eat?</td>\n",
       "      <td>Grasshopper</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([0]), 'answer_text': ar...</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grasshopper</td>\n",
       "      <td>['What', 'do', 'Grasshoppers', 'eat', '?']</td>\n",
       "      <td>['Grasshoppers', 'are', 'plant-eaters', ',', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question_text  \\\n",
       "0           When was quantum field theory developed?   \n",
       "1  Who was the first Nobel prize winner for Liter...   \n",
       "2               When is the dialectical method used?   \n",
       "3                               Who invented Hangul?   \n",
       "4                          What do Grasshoppers eat?   \n",
       "\n",
       "                          document_title language  \\\n",
       "0                   Quantum field theory  english   \n",
       "1  List of Nobel laureates in Literature  english   \n",
       "2                              Dialectic  english   \n",
       "3                       Origin of Hangul  english   \n",
       "4                            Grasshopper  english   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  {'answer_start': array([159]), 'answer_text': ...   \n",
       "1  {'answer_start': array([610]), 'answer_text': ...   \n",
       "2  {'answer_start': array([129]), 'answer_text': ...   \n",
       "3  {'answer_start': array([88]), 'answer_text': a...   \n",
       "4  {'answer_start': array([0]), 'answer_text': ar...   \n",
       "\n",
       "                                  document_plaintext  \\\n",
       "0  Quantum field theory naturally began with the ...   \n",
       "1  The Nobel Prize in Literature (Swedish: Nobelp...   \n",
       "2  Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
       "3  Hangul was personally created and promulgated ...   \n",
       "4  Grasshoppers are plant-eaters, with a few spec...   \n",
       "\n",
       "                                        document_url  \\\n",
       "0  https://en.wikipedia.org/wiki/Quantum%20field%...   \n",
       "1  https://en.wikipedia.org/wiki/List%20of%20Nobe...   \n",
       "2            https://en.wikipedia.org/wiki/Dialectic   \n",
       "3  https://en.wikipedia.org/wiki/Origin%20of%20Ha...   \n",
       "4          https://en.wikipedia.org/wiki/Grasshopper   \n",
       "\n",
       "                             question_text_tokenized  \\\n",
       "0  ['When', 'was', 'quantum', 'field', 'theory', ...   \n",
       "1  ['Who', 'was', 'the', 'first', 'Nobel', 'prize...   \n",
       "2  ['When', 'is', 'the', 'dialectical', 'method',...   \n",
       "3                 ['Who', 'invented', 'Hangul', '?']   \n",
       "4         ['What', 'do', 'Grasshoppers', 'eat', '?']   \n",
       "\n",
       "                        document_plaintext_tokenized  label  \\\n",
       "0  ['Quantum', 'field', 'theory', 'naturally', 'b...      1   \n",
       "1  ['The', 'Nobel', 'Prize', 'in', 'Literature', ...      1   \n",
       "2  ['Dialectic', 'or', 'dialectics', '(', 'Greek'...      1   \n",
       "3  ['Hangul', 'was', 'personally', 'created', 'an...      1   \n",
       "4  ['Grasshoppers', 'are', 'plant-eaters', ',', '...      1   \n",
       "\n",
       "   word_frequency_score  logres_pred  \n",
       "0              0.428571            1  \n",
       "1              0.700000            1  \n",
       "2              0.571429            1  \n",
       "3              0.250000            1  \n",
       "4              0.200000            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import training data\n",
    "dft_eng = pd.read_csv('../../data/dft_eng.csv')\n",
    "dft_jap = pd.read_csv('../../data/dft_jap.csv')\n",
    "dft_fin = pd.read_csv('../../data/dft_fin.csv')\n",
    "\n",
    "# import validation data\n",
    "dfv_eng = pd.read_csv('../../data/dfv_eng.csv')\n",
    "dfv_jap = pd.read_csv('../../data/dfv_jap.csv')\n",
    "dfv_fin = pd.read_csv('../../data/dfv_fin.csv')\n",
    "\n",
    "#import word count\n",
    "word_count = pd.read_csv('../../data/question_word_count.csv')\n",
    "\n",
    "dft_eng.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text, model, tokenizer, max_length=100):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model(input_ids)\n",
    "    return output.hidden_states\n",
    "\n",
    "def generate_all(text, model, tokenizer, max_length, num_beams, no_reapeat_ngrams, temperature, top_k, top_p):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    greedy_output = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "    beam_search_output = model.generate(input_ids, max_length=max_length, do_sample=True, num_beams=num_beams)\n",
    "    n_grams_output = model.generate(input_ids, max_length=max_length, no_repeat_ngram_size=no_reapeat_ngrams, num_beams=num_beams)\n",
    "    sample_output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature)\n",
    "    top_k_otput = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=top_k)\n",
    "    top_p_output = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=top_k, top_p=top_p)\n",
    "\n",
    "    output_lst = [greedy_output, beam_search_output, n_grams_output, sample_output, top_k_otput, top_p_output]\n",
    "    decoded_samples = [tokenizer.decode(g[0], skip_special_tokens=True) for g in output_lst]\n",
    "\n",
    "    return decoded_samples\n",
    "\n",
    "def generate_top_p(text, model, tokenizer, max_length, top_p, top_k):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, do_sample=True,top_k=top_k, top_p=top_p)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_sample(text, model, tokenizer, max_length, temperature):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was quantum field theory developed? Is quantum theory actually better than the current, or what could you guess?\n",
      "\n",
      "\n",
      "\n",
      "I think Quantum Theory can be improved in the future, but the question is \"what about the next big thing? How long will it take to be able to really understand quantum theory?\"\n",
      "Here are some more answers:\n",
      "Narrow Quantum Theory\n",
      "Q3: Does Quantum Theory ever be realized?\n",
      "Q4: It will.\n",
      "Q5: Yes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = generate_top_p(dft_eng.question_text[0], model, tokenizer, 100, 0.9, 50)\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0301,  0.3668,  0.0901,  ..., -0.2221,  0.1273, -0.1497],\n",
       "         [ 0.5815,  0.2135,  0.1955,  ...,  0.4937,  0.0897, -0.2369],\n",
       "         [ 0.2250,  0.5953, -0.3460,  ...,  0.3857, -0.0740,  0.1617]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate_top_p(\"The weather is\", model, tokenizer, 50, 0.95, 50)\n",
    "get_hidden_states(\"The weather is\", model, tokenizer)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('I was meaning to', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (C:/Users/Hallgrimur/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b7775404bd43c0aa889681fca01551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.to_pandas()\n",
    "validation_set = validation_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_chosen = [\"english\"]\n",
    "\n",
    "train_set = train_set[train_set['language'].isin(languages_chosen)]\n",
    "validation_set = validation_set[validation_set['language'].isin(languages_chosen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_text', 'document_title', 'language', 'annotations',\n",
       "       'document_plaintext', 'document_url', 'answerable'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_answerability(annotations):\n",
    "    answerable = []\n",
    "    for annot in annotations:\n",
    "        if -1 in annot['answer_start']:\n",
    "            answerable.append(0)\n",
    "        else:\n",
    "            answerable.append(1)\n",
    "    return answerable\n",
    "\n",
    "train_annotations = train_set['annotations'].tolist()\n",
    "validation_annotations = validation_set['annotations'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "train_set['answerable'] = get_answerability(train_annotations)\n",
    "validation_set['answerable'] = get_answerability(validation_annotations)\n",
    "train_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.drop(columns=['annotations', 'document_url'])\n",
    "validation_set = validation_set.drop(columns=['annotations', 'document_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_parag_combine(questions, paragraphs):\n",
    "    \"\"\"\n",
    "    This function combines the questions and paragraphs into a single text\n",
    "    Args:\n",
    "        questions: list of questions\n",
    "        paragraphs: list of paragraphs\n",
    "    Returns:\n",
    "        list of combined questions and paragraphs\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    for index in range(len(questions)):\n",
    "        training_data += [questions[index] + \"\\n\" + paragraphs[index]]\n",
    "        \n",
    "    return training_data\n",
    "\n",
    "def get_data_with_cond(data_set, cond, vectorizer):\n",
    "    \"\"\"\n",
    "    This function returns the data with the given condition (can be used to get data for a particular language).\n",
    "    vectorizer is used to vectorize the data: it can be a CountVectorizer or a TfidfVectorizer, etc.\n",
    "    If vectorizer is None, then the combined data is returned as is.\n",
    "    Args:\n",
    "        data_set: pandas dataframe\n",
    "        cond: condition to be applied\n",
    "        vectorizer: vectorizer to be used\n",
    "    Returns:\n",
    "        data with the given condition\n",
    "    \"\"\"\n",
    "\n",
    "    d_q = data_set[cond]['question_text'].tolist()\n",
    "    d_p = data_set[cond]['document_plaintext'].tolist()\n",
    "    data = question_parag_combine(d_q,d_p)\n",
    "\n",
    "    print(len(d_q))\n",
    "    if vectorizer is None:\n",
    "        return data \n",
    "    \n",
    "    X = vectorizer.transform(data)\n",
    "    y = data_set[cond]['answerable'].tolist()\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "#example use \n",
    "# cond_eng = validation_set['language'] == 'english'\n",
    "# X_eng, y_eng = get_data_with_cond(validation_set, cond_eng, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "model_checkpoint = 'distilgpt2'\n",
    "max_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, padding = 'max_length', max_length=max_length, use_fast=True, truncation=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "d_q = train_set['question_text'].tolist()\n",
    "d_p = train_set['document_plaintext'].tolist()\n",
    "training_data = question_parag_combine(d_q,d_p)\n",
    "training_labels = train_set['answerable'].tolist()\n",
    "\n",
    "d_q = validation_set['question_text'].tolist()\n",
    "d_p = validation_set['document_plaintext'].tolist()\n",
    "validation_data = question_parag_combine(d_q,d_p)\n",
    "validation_labels = validation_set['answerable'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "data_set = {}\n",
    "sets = [['train',training_data, training_labels], ['val',validation_data, validation_labels]]\n",
    "for meta in sets:\n",
    "    data_set[meta[0]] = {}\n",
    "    data_set[meta[0]]['text'] = []\n",
    "    data_set[meta[0]]['label'] = []\n",
    "    \n",
    "    for ind, text in enumerate(meta[1]):\n",
    "        data_set[meta[0]]['text'].append(text)\n",
    "        data_set[meta[0]]['label'].append(meta[2][ind])\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "data_set = DatasetDict({'train':Dataset.from_dict(data_set['train']),\n",
    "                        'valid':Dataset.from_dict(data_set['val'])\\\n",
    "                       })\n",
    "\n",
    "# training_data = tokenize_data(training_data)\n",
    "#  validation_data = tokenize_data(validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe8f1bc96a24d0aa98de59314537f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35645efada64e11bc02feb4095b0645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    keys = ['attention_mask', 'input_ids']\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in keys}\n",
    "    total_length = len(concatenated_examples[list(keys)[0]])\n",
    "    print(total_length)\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # this is needed as the used dataset is a subclass of ClassificationDataset, which requires label as a field...\n",
    "    result[\"label\"] = result[\"input_ids\"].copy()\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "    \n",
    "tokenized_datasets = data_set.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7513e8eab074f72b4e0ac4b52cde0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147120\n",
      "145451\n",
      "150850\n",
      "143694\n",
      "137237\n",
      "133840\n",
      "132161\n",
      "54478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2ddf0fab1c45fc99d6f7dfaa312bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142088\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "tokenized_datasets_lm = tokenized_datasets.map(group_texts, batched=True, batch_size=1000,)\n",
    "tokenized_datasets_lm = tokenized_datasets_lm.remove_columns([\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 7389\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilgpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "print(model_name)\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_lm[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_lm[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2215, 373, 14821, 2214, 4583, 4166, 30, 198, 24915, 388, 2214, 4583, 8752, 2540, 351, 262, 2050, 286, 31094, 12213, 11, 355, 262, 31094, 2214, 373, 262, 691, 1900, 15993, 2214, 355, 286, 262, 14062, 82, 3693, 23, 5974, 16, 8241, 373, 262, 717, 20715, 11596, 8464, 329, 33818, 30, 198, 464, 20715, 15895, 287, 33818, 357, 10462, 276, 680, 25, 20715, 1050, 271, 316, 1312, 25359, 2541, 8, 318, 11343, 13844, 416, 262, 14023, 8581, 284, 7035, 329, 11660, 9284, 287, 262, 2214, 286, 9285, 13, 632, 318, 530, 286, 262, 1936, 20715, 4389, 12271, 4920, 416, 262, 46425, 481, 286, 22044, 20715, 11, 543, 389, 11343, 329, 11660, 9284, 287, 16585, 11, 11887, 11, 9285, 11, 4167, 11, 290, 38033, 393, 9007, 3693, 16, 60, 1081], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2215, 373, 14821, 2214, 4583, 4166, 30, 198, 24915, 388, 2214, 4583, 8752, 2540, 351, 262, 2050, 286, 31094, 12213, 11, 355, 262, 31094, 2214, 373, 262, 691, 1900, 15993, 2214, 355, 286, 262, 14062, 82, 3693, 23, 5974, 16, 8241, 373, 262, 717, 20715, 11596, 8464, 329, 33818, 30, 198, 464, 20715, 15895, 287, 33818, 357, 10462, 276, 680, 25, 20715, 1050, 271, 316, 1312, 25359, 2541, 8, 318, 11343, 13844, 416, 262, 14023, 8581, 284, 7035, 329, 11660, 9284, 287, 262, 2214, 286, 9285, 13, 632, 318, 530, 286, 262, 1936, 20715, 4389, 12271, 4920, 416, 262, 46425, 481, 286, 22044, 20715, 11, 543, 389, 11343, 329, 11660, 9284, 287, 16585, 11, 11887, 11, 9285, 11, 4167, 11, 290, 38033, 393, 9007, 3693, 16, 60, 1081]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets_lm['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 990\n",
      "  Batch size = 8\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mevaluate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerplexity: \u001b[39m\u001b[39m{\u001b[39;00mmath\u001b[39m.\u001b[39mexp(eval_results[\u001b[39m'\u001b[39m\u001b[39meval_loss\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2281\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2283\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2284\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2285\u001b[0m     eval_dataloader,\n\u001b[0;32m   2286\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2287\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2288\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2289\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2290\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2291\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2292\u001b[0m )\n\u001b[0;32m   2294\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2295\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2296\u001b[0m     speed_metrics(\n\u001b[0;32m   2297\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     )\n\u001b[0;32m   2302\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2448\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2446\u001b[0m observed_num_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   2447\u001b[0m \u001b[39m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 2448\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m   2449\u001b[0m     \u001b[39m# Update the observed num examples\u001b[39;00m\n\u001b[0;32m   2450\u001b[0m     observed_batch_size \u001b[39m=\u001b[39m find_batch_size(inputs)\n\u001b[0;32m   2451\u001b[0m     \u001b[39mif\u001b[39;00m observed_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:41\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[0;32m     40\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[0;32m     42\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:729\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[1;34m(self, examples)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtorch_call\u001b[39m(\u001b[39mself\u001b[39m, examples: List[Union[List[\u001b[39mint\u001b[39m], Any, Dict[\u001b[39mstr\u001b[39m, Any]]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m    727\u001b[0m     \u001b[39m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(examples[\u001b[39m0\u001b[39m], (\u001b[39mdict\u001b[39m, BatchEncoding)):\n\u001b[1;32m--> 729\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(examples, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of)\n\u001b[0;32m    730\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m         batch \u001b[39m=\u001b[39m {\n\u001b[0;32m    732\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: _torch_collate_batch(examples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_to_multiple_of)\n\u001b[0;32m    733\u001b[0m         }\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2814\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2811\u001b[0m         encoded_inputs[key] \u001b[39m=\u001b[39m to_py_obj(value)\n\u001b[0;32m   2813\u001b[0m \u001b[39m# Convert padding_strategy in PaddingStrategy\u001b[39;00m\n\u001b[1;32m-> 2814\u001b[0m padding_strategy, _, max_length, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[0;32m   2815\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding, max_length\u001b[39m=\u001b[39;49mmax_length, verbose\u001b[39m=\u001b[39;49mverbose\n\u001b[0;32m   2816\u001b[0m )\n\u001b[0;32m   2818\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[0;32m   2819\u001b[0m \u001b[39mif\u001b[39;00m required_input \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(required_input[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2350\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[1;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2348\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[0;32m   2349\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m-> 2350\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2351\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2352\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2353\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2354\u001b[0m     )\n\u001b[0;32m   2356\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[0;32m   2357\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2358\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[0;32m   2359\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2362\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m   2363\u001b[0m ):\n",
      "\u001b[1;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"document_plaintext\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    keys = ['attention_mask', 'input_ids']\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in keys}\n",
    "    total_length = len(concatenated_examples[list(keys)[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # this is needed as the used dataset is a subclass of ClassificationDataset, which requires label as a field...\n",
    "    result[\"label\"] = result[\"input_ids\"].copy()\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 'The previous mayor, Bill Laforet faced a recall election in November 2018, after a resident group submitted in June a list of 5,000 petition signatures that they had collected calling for the action, in excess of the 25% needed to place the measure in front of voters.[85] In the November 2018 general election, Laforet was recalled from office and John Roth was elected mayor. The successful recall was the first in the county for at least 25 years.[86]'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = {'train': dft_eng.document_plaintext[x] for x in range(len(dft_eng.document_plaintext))}\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m textlst \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(dft_eng\u001b[39m.\u001b[39mdocument_plaintext)), \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train \u001b[39m=\u001b[39m {textlst[x]: dft_eng\u001b[39m.\u001b[39mdocument_plaintext[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dft_eng\u001b[39m.\u001b[39mdocument_plaintext))}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train\n",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 26\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m textlst \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(dft_eng\u001b[39m.\u001b[39mdocument_plaintext)), \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train \u001b[39m=\u001b[39m {textlst[x]: dft_eng\u001b[39m.\u001b[39mdocument_plaintext[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dft_eng\u001b[39m.\u001b[39mdocument_plaintext))}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "textlst = np.full((1, len(dft_eng.document_plaintext)), 'text').tolist()\n",
    "train = {textlst[x]: dft_eng.document_plaintext[x] for x in range(len(dft_eng.document_plaintext))}\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokonizedt = [tokenizer(x) for x in dft_eng.question_text]\n",
    "tokonizedv = [tokenizer(x) for x in dfv_eng.question_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokonizedt,\n",
    "    eval_dataset=tokonizedv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 990\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 11 at dim 1 (got 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mevaluate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerplexity: \u001b[39m\u001b[39m{\u001b[39;00mmath\u001b[39m.\u001b[39mexp(eval_results[\u001b[39m'\u001b[39m\u001b[39meval_loss\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2281\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2283\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2284\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2285\u001b[0m     eval_dataloader,\n\u001b[0;32m   2286\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2287\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2288\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2289\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2290\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2291\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2292\u001b[0m )\n\u001b[0;32m   2294\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2295\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2296\u001b[0m     speed_metrics(\n\u001b[0;32m   2297\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     )\n\u001b[0;32m   2302\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2448\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2446\u001b[0m observed_num_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   2447\u001b[0m \u001b[39m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 2448\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m   2449\u001b[0m     \u001b[39m# Update the observed num examples\u001b[39;00m\n\u001b[0;32m   2450\u001b[0m     observed_batch_size \u001b[39m=\u001b[39m find_batch_size(inputs)\n\u001b[0;32m   2451\u001b[0m     \u001b[39mif\u001b[39;00m observed_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     67\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:130\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    128\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features])\n\u001b[0;32m    129\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[0;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 11 at dim 1 (got 10)"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tydiqa (C:/Users/Hallgrimur/.cache/huggingface/datasets/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67014aa117584c11bc947336beb03bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = load_dataset('tydiqa', 'primary_task')\n",
    "#unsupervised_imdb_splits = unsupervised_imdb.train_test_split(test_size=0.01)\n",
    "#print(unsupervised_imdb_splits.keys())\n",
    "#print(unsupervised_imdb_splits['train'][0])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage_answer_candidates</th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'plaintext_start_byte': [1, 660, 844, 1196, 1...</td>\n",
       "      <td>berapakah jenis ras yang ada didunia?</td>\n",
       "      <td>Ras manusia</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\ntransl.\\n\\nRas (dari bahasa Prancis race, ya...</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Ras%20manusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'plaintext_start_byte': [1, 271, 995, 1763, 2...</td>\n",
       "      <td>2018年アメリカで一番治安の悪い州はどこ</td>\n",
       "      <td>デトロイト</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n\\nデトロイト（ /dɨˈtrɔɪt/）は、アメリカ合衆国ミシガン州南東部にある都市...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'plaintext_start_byte': [0, 208, 542, 891, 10...</td>\n",
       "      <td>Je,Ngamia anaweza kaa bila maji kwa muda gani?</td>\n",
       "      <td>Kuku</td>\n",
       "      <td>swahili</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\nKuku (Gallus gallus domesticus) ni ndege ana...</td>\n",
       "      <td>https://sw.wikipedia.org/wiki/Kuku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'plaintext_start_byte': [5, 401, 1185, 2103, ...</td>\n",
       "      <td>কম্পিউটার বিজ্ঞানের মোট কয়টি শাখা রয়েছে ?</td>\n",
       "      <td>বিজ্ঞান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nভৌত বিশ্বের যা কিছু পর্যবেক্ষণ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'plaintext_start_byte': [0, 395, 716, 1320, 3...</td>\n",
       "      <td>మెదక్ నగర విస్తీర్ణం ఎంత?</td>\n",
       "      <td>మెదక్ జిల్లా</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>మెదక్ జిల్లా తెలంగాణ రాష్ట్రంలోని 31 జిల్లాలలో...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%AE%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>{'plaintext_start_byte': [1, 1142, 3214, 3713,...</td>\n",
       "      <td>మొట్టమొదటి కెమెరా పేరేమిటి ?</td>\n",
       "      <td>కెమెరా</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\nకెమెరా (ఆంగ్లం: Camera) అనగా స్థిర చిత్రాలను...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%95%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>{'plaintext_start_byte': [0, 249, 541, 1064, 1...</td>\n",
       "      <td>大和民族より前に日本列島に住んでいた民族はいる？</td>\n",
       "      <td>大和民族</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'passage_answer_candidate_index': [37], 'mini...</td>\n",
       "      <td>\\n\\n大和民族（やまとみんぞく）は、日本列島の住民の大半を占める民族である。ほとんどが日本...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E5%A4%A7%E5%92%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>{'plaintext_start_byte': [0, 378, 457, 1348, 1...</td>\n",
       "      <td>كم عدد آيات سورة الحديد؟</td>\n",
       "      <td>سورة الحديد</td>\n",
       "      <td>arabic</td>\n",
       "      <td>{'passage_answer_candidate_index': [0], 'minim...</td>\n",
       "      <td>\\nسورة الحديد  هي سورة مدنية عدد آياتها 29 وتر...</td>\n",
       "      <td>https://ar.wikipedia.org/wiki/%D8%B3%D9%88%D8%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>{'plaintext_start_byte': [0, 544, 675, 1140, 1...</td>\n",
       "      <td>Kuinka monta romaania Stephen King on kirjoitt...</td>\n",
       "      <td>Stephen King</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n\\nStephen Edwin King (s. 21. syyskuuta 194...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Stephen%20King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>{'plaintext_start_byte': [2, 510, 1094, 1489, ...</td>\n",
       "      <td>2018년에 가장 오래된 시계는 무엇인가?</td>\n",
       "      <td>시계</td>\n",
       "      <td>korean</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n시계(時計, )는 시간을 나타내거나 시간을 재는 기계나 장치이다. 시계에는 ...</td>\n",
       "      <td>https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B3%84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               passage_answer_candidates  \\\n",
       "0      {'plaintext_start_byte': [1, 660, 844, 1196, 1...   \n",
       "1      {'plaintext_start_byte': [1, 271, 995, 1763, 2...   \n",
       "2      {'plaintext_start_byte': [0, 208, 542, 891, 10...   \n",
       "3      {'plaintext_start_byte': [5, 401, 1185, 2103, ...   \n",
       "4      {'plaintext_start_byte': [0, 395, 716, 1320, 3...   \n",
       "...                                                  ...   \n",
       "99995  {'plaintext_start_byte': [1, 1142, 3214, 3713,...   \n",
       "99996  {'plaintext_start_byte': [0, 249, 541, 1064, 1...   \n",
       "99997  {'plaintext_start_byte': [0, 378, 457, 1348, 1...   \n",
       "99998  {'plaintext_start_byte': [0, 544, 675, 1140, 1...   \n",
       "99999  {'plaintext_start_byte': [2, 510, 1094, 1489, ...   \n",
       "\n",
       "                                           question_text document_title  \\\n",
       "0                  berapakah jenis ras yang ada didunia?    Ras manusia   \n",
       "1                                  2018年アメリカで一番治安の悪い州はどこ          デトロイト   \n",
       "2         Je,Ngamia anaweza kaa bila maji kwa muda gani?           Kuku   \n",
       "3              কম্পিউটার বিজ্ঞানের মোট কয়টি শাখা রয়েছে ?        বিজ্ঞান   \n",
       "4                              మెదక్ నగర విస్తీర్ణం ఎంత?   మెదక్ జిల్లా   \n",
       "...                                                  ...            ...   \n",
       "99995                       మొట్టమొదటి కెమెరా పేరేమిటి ?         కెమెరా   \n",
       "99996                           大和民族より前に日本列島に住んでいた民族はいる？           大和民族   \n",
       "99997                           كم عدد آيات سورة الحديد؟    سورة الحديد   \n",
       "99998  Kuinka monta romaania Stephen King on kirjoitt...   Stephen King   \n",
       "99999                            2018년에 가장 오래된 시계는 무엇인가?             시계   \n",
       "\n",
       "         language                                        annotations  \\\n",
       "0      indonesian  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "1        japanese  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "2         swahili  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "3         bengali  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "4          telugu  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "...           ...                                                ...   \n",
       "99995      telugu  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "99996    japanese  {'passage_answer_candidate_index': [37], 'mini...   \n",
       "99997      arabic  {'passage_answer_candidate_index': [0], 'minim...   \n",
       "99998     finnish  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "99999      korean  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "\n",
       "                                      document_plaintext  \\\n",
       "0      \\ntransl.\\n\\nRas (dari bahasa Prancis race, ya...   \n",
       "1      \\n\\n\\nデトロイト（ /dɨˈtrɔɪt/）は、アメリカ合衆国ミシガン州南東部にある都市...   \n",
       "2      \\nKuku (Gallus gallus domesticus) ni ndege ana...   \n",
       "3      \\n\\n\\n\\n\\n\\n\\n\\nভৌত বিশ্বের যা কিছু পর্যবেক্ষণ...   \n",
       "4      మెదక్ జిల్లా తెలంగాణ రాష్ట్రంలోని 31 జిల్లాలలో...   \n",
       "...                                                  ...   \n",
       "99995  \\nకెమెరా (ఆంగ్లం: Camera) అనగా స్థిర చిత్రాలను...   \n",
       "99996  \\n\\n大和民族（やまとみんぞく）は、日本列島の住民の大半を占める民族である。ほとんどが日本...   \n",
       "99997  \\nسورة الحديد  هي سورة مدنية عدد آياتها 29 وتر...   \n",
       "99998  \\n\\n\\nStephen Edwin King (s. 21. syyskuuta 194...   \n",
       "99999  \\n\\n시계(時計, )는 시간을 나타내거나 시간을 재는 기계나 장치이다. 시계에는 ...   \n",
       "\n",
       "                                            document_url  \n",
       "0            https://id.wikipedia.org/wiki/Ras%20manusia  \n",
       "1      https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%...  \n",
       "2                     https://sw.wikipedia.org/wiki/Kuku  \n",
       "3      https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%...  \n",
       "4      https://te.wikipedia.org/wiki/%E0%B0%AE%E0%B1%...  \n",
       "...                                                  ...  \n",
       "99995  https://te.wikipedia.org/wiki/%E0%B0%95%E0%B1%...  \n",
       "99996  https://ja.wikipedia.org/wiki/%E5%A4%A7%E5%92%...  \n",
       "99997  https://ar.wikipedia.org/wiki/%D8%B3%D9%88%D8%...  \n",
       "99998       https://fi.wikipedia.org/wiki/Stephen%20King  \n",
       "99999   https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B3%84  \n",
       "\n",
       "[100000 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame.from_dict(train['train'][0:100000]) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage_answer_candidates</th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'plaintext_start_byte': [2, 740, 1381, 1941, ...</td>\n",
       "      <td>When did the art deco movement begin?</td>\n",
       "      <td>Art Deco</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\nArt Deco, sometimes referred to as Deco, i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Art%20Deco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'plaintext_start_byte': [5, 378, 956, 1342, 3...</td>\n",
       "      <td>Is Creole a pidgin of French?</td>\n",
       "      <td>French-based creole languages</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [1], 'minim...</td>\n",
       "      <td>\\n\\n\\n\\n\\nPart of a series on theFrench langua...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/French-based%20c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>{'plaintext_start_byte': [7, 1637, 1938, 2380,...</td>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [12], 'mini...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nQuantum field theoryFeynman diag...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>{'plaintext_start_byte': [2, 284, 580, 837, 10...</td>\n",
       "      <td>What was the highest value of the yen in 2018?</td>\n",
       "      <td>Banknotes of the Japanese yen</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\nThe banknotes of the Japanese yen are part...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Banknotes%20of%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>{'plaintext_start_byte': [3, 118, 357, 1045, 1...</td>\n",
       "      <td>Does plastic decompose at all?</td>\n",
       "      <td>Biodegradable plastic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [0], 'minim...</td>\n",
       "      <td>\\n\\n\\nBiodegradable plastics are plastics that...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Biodegradable%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99899</th>\n",
       "      <td>{'plaintext_start_byte': [3, 517, 858, 1228, 1...</td>\n",
       "      <td>When was ultrasound first used in medicine?</td>\n",
       "      <td>Medical ultrasound</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [59], 'mini...</td>\n",
       "      <td>\\n\\n\\nMedical ultrasound (also known as diagno...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Medical%20ultras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99917</th>\n",
       "      <td>{'plaintext_start_byte': [0, 171, 498, 868, 15...</td>\n",
       "      <td>Do steam locomotives have gears?</td>\n",
       "      <td>Geared steam locomotive</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [0], 'minim...</td>\n",
       "      <td>A geared steam locomotive is a type of steam l...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Geared%20steam%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99942</th>\n",
       "      <td>{'plaintext_start_byte': [1, 459, 636, 1198, 1...</td>\n",
       "      <td>When was the West Virginia Mountaineers basket...</td>\n",
       "      <td>West Virginia Mountaineers men's basketball</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [4], 'minim...</td>\n",
       "      <td>\\nThe West Virginia Mountaineers men's basketb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/West%20Virginia%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99943</th>\n",
       "      <td>{'plaintext_start_byte': [3, 595, 1443, 2355, ...</td>\n",
       "      <td>How long does it take solar wind to reach the ...</td>\n",
       "      <td>Solar wind</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n\\nThe solar wind is a stream of charged pa...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Solar%20wind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99962</th>\n",
       "      <td>{'plaintext_start_byte': [0, 460, 1755, 3058, ...</td>\n",
       "      <td>How much does it cost to adopt a baby?</td>\n",
       "      <td>Adoption in California</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>More adoptions occur in California each year t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adoption%20in%20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5540 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               passage_answer_candidates  \\\n",
       "28     {'plaintext_start_byte': [2, 740, 1381, 1941, ...   \n",
       "50     {'plaintext_start_byte': [5, 378, 956, 1342, 3...   \n",
       "65     {'plaintext_start_byte': [7, 1637, 1938, 2380,...   \n",
       "76     {'plaintext_start_byte': [2, 284, 580, 837, 10...   \n",
       "87     {'plaintext_start_byte': [3, 118, 357, 1045, 1...   \n",
       "...                                                  ...   \n",
       "99899  {'plaintext_start_byte': [3, 517, 858, 1228, 1...   \n",
       "99917  {'plaintext_start_byte': [0, 171, 498, 868, 15...   \n",
       "99942  {'plaintext_start_byte': [1, 459, 636, 1198, 1...   \n",
       "99943  {'plaintext_start_byte': [3, 595, 1443, 2355, ...   \n",
       "99962  {'plaintext_start_byte': [0, 460, 1755, 3058, ...   \n",
       "\n",
       "                                           question_text  \\\n",
       "28                 When did the art deco movement begin?   \n",
       "50                         Is Creole a pidgin of French?   \n",
       "65              When was quantum field theory developed?   \n",
       "76        What was the highest value of the yen in 2018?   \n",
       "87                        Does plastic decompose at all?   \n",
       "...                                                  ...   \n",
       "99899        When was ultrasound first used in medicine?   \n",
       "99917                   Do steam locomotives have gears?   \n",
       "99942  When was the West Virginia Mountaineers basket...   \n",
       "99943  How long does it take solar wind to reach the ...   \n",
       "99962             How much does it cost to adopt a baby?   \n",
       "\n",
       "                                    document_title language  \\\n",
       "28                                        Art Deco  english   \n",
       "50                   French-based creole languages  english   \n",
       "65                            Quantum field theory  english   \n",
       "76                   Banknotes of the Japanese yen  english   \n",
       "87                           Biodegradable plastic  english   \n",
       "...                                            ...      ...   \n",
       "99899                           Medical ultrasound  english   \n",
       "99917                      Geared steam locomotive  english   \n",
       "99942  West Virginia Mountaineers men's basketball  english   \n",
       "99943                                   Solar wind  english   \n",
       "99962                       Adoption in California  english   \n",
       "\n",
       "                                             annotations  \\\n",
       "28     {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "50     {'passage_answer_candidate_index': [1], 'minim...   \n",
       "65     {'passage_answer_candidate_index': [12], 'mini...   \n",
       "76     {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "87     {'passage_answer_candidate_index': [0], 'minim...   \n",
       "...                                                  ...   \n",
       "99899  {'passage_answer_candidate_index': [59], 'mini...   \n",
       "99917  {'passage_answer_candidate_index': [0], 'minim...   \n",
       "99942  {'passage_answer_candidate_index': [4], 'minim...   \n",
       "99943  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "99962  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "\n",
       "                                      document_plaintext  \\\n",
       "28     \\n\\nArt Deco, sometimes referred to as Deco, i...   \n",
       "50     \\n\\n\\n\\n\\nPart of a series on theFrench langua...   \n",
       "65     \\n\\n\\n\\n\\n\\n\\nQuantum field theoryFeynman diag...   \n",
       "76     \\n\\nThe banknotes of the Japanese yen are part...   \n",
       "87     \\n\\n\\nBiodegradable plastics are plastics that...   \n",
       "...                                                  ...   \n",
       "99899  \\n\\n\\nMedical ultrasound (also known as diagno...   \n",
       "99917  A geared steam locomotive is a type of steam l...   \n",
       "99942  \\nThe West Virginia Mountaineers men's basketb...   \n",
       "99943  \\n\\n\\nThe solar wind is a stream of charged pa...   \n",
       "99962  More adoptions occur in California each year t...   \n",
       "\n",
       "                                            document_url  \n",
       "28              https://en.wikipedia.org/wiki/Art%20Deco  \n",
       "50     https://en.wikipedia.org/wiki/French-based%20c...  \n",
       "65     https://en.wikipedia.org/wiki/Quantum%20field%...  \n",
       "76     https://en.wikipedia.org/wiki/Banknotes%20of%2...  \n",
       "87     https://en.wikipedia.org/wiki/Biodegradable%20...  \n",
       "...                                                  ...  \n",
       "99899  https://en.wikipedia.org/wiki/Medical%20ultras...  \n",
       "99917  https://en.wikipedia.org/wiki/Geared%20steam%2...  \n",
       "99942  https://en.wikipedia.org/wiki/West%20Virginia%...  \n",
       "99943         https://en.wikipedia.org/wiki/Solar%20wind  \n",
       "99962  https://en.wikipedia.org/wiki/Adoption%20in%20...  \n",
       "\n",
       "[5540 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['language'] == 'english']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85ac075dbf107ca10f05bcc388d88f275addd2a735b233d21443c8ee15d0b537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
