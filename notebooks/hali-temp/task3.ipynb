{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from bpemb import BPEmb\n",
    "from tqdm import tqdm\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>question_text_tokenized</th>\n",
       "      <th>document_plaintext_tokenized</th>\n",
       "      <th>answer_text_tokenized</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([159]), 'answer_text': ...</td>\n",
       "      <td>Quantum field theory naturally began with the ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "      <td>[159]</td>\n",
       "      <td>['1920s']</td>\n",
       "      <td>['when', 'was', 'quantum', 'field', 'theory', ...</td>\n",
       "      <td>['quantum', 'field', 'theory', 'naturally', 'b...</td>\n",
       "      <td>['1920s']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first Nobel prize winner for Liter...</td>\n",
       "      <td>List of Nobel laureates in Literature</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([610]), 'answer_text': ...</td>\n",
       "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Nobe...</td>\n",
       "      <td>[610]</td>\n",
       "      <td>['Sully Prudhomme']</td>\n",
       "      <td>['who', 'was', 'the', 'first', 'nobel', 'prize...</td>\n",
       "      <td>['the', 'nobel', 'prize', 'in', 'literature', ...</td>\n",
       "      <td>['sully', 'prudhomme']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When is the dialectical method used?</td>\n",
       "      <td>Dialectic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([129]), 'answer_text': ...</td>\n",
       "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dialectic</td>\n",
       "      <td>[129]</td>\n",
       "      <td>['discourse between two or more people holding...</td>\n",
       "      <td>['when', 'is', 'the', 'dialectical', 'method',...</td>\n",
       "      <td>['dialectic', 'or', 'dialectics', '(', 'greek'...</td>\n",
       "      <td>['discourse', 'between', 'two', 'or', 'more', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who invented Hangul?</td>\n",
       "      <td>Origin of Hangul</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([88]), 'answer_text': a...</td>\n",
       "      <td>Hangul was personally created and promulgated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Origin%20of%20Ha...</td>\n",
       "      <td>[88]</td>\n",
       "      <td>['Sejong the Great']</td>\n",
       "      <td>['who', 'invented', 'hangul', '?']</td>\n",
       "      <td>['hangul', 'was', 'personally', 'created', 'an...</td>\n",
       "      <td>['sejong', 'the', 'great']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do Grasshoppers eat?</td>\n",
       "      <td>Grasshopper</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([0]), 'answer_text': ar...</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grasshopper</td>\n",
       "      <td>[0]</td>\n",
       "      <td>['Grasshoppers are plant-eaters, with a few sp...</td>\n",
       "      <td>['what', 'do', 'grasshoppers', 'eat', '?']</td>\n",
       "      <td>['grasshoppers', 'are', 'plant-eaters', ',', '...</td>\n",
       "      <td>['grasshoppers', 'are', 'plant-eaters', ',', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question_text  \\\n",
       "0           When was quantum field theory developed?   \n",
       "1  Who was the first Nobel prize winner for Liter...   \n",
       "2               When is the dialectical method used?   \n",
       "3                               Who invented Hangul?   \n",
       "4                          What do Grasshoppers eat?   \n",
       "\n",
       "                          document_title language  \\\n",
       "0                   Quantum field theory  english   \n",
       "1  List of Nobel laureates in Literature  english   \n",
       "2                              Dialectic  english   \n",
       "3                       Origin of Hangul  english   \n",
       "4                            Grasshopper  english   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  {'answer_start': array([159]), 'answer_text': ...   \n",
       "1  {'answer_start': array([610]), 'answer_text': ...   \n",
       "2  {'answer_start': array([129]), 'answer_text': ...   \n",
       "3  {'answer_start': array([88]), 'answer_text': a...   \n",
       "4  {'answer_start': array([0]), 'answer_text': ar...   \n",
       "\n",
       "                                  document_plaintext  \\\n",
       "0  Quantum field theory naturally began with the ...   \n",
       "1  The Nobel Prize in Literature (Swedish: Nobelp...   \n",
       "2  Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
       "3  Hangul was personally created and promulgated ...   \n",
       "4  Grasshoppers are plant-eaters, with a few spec...   \n",
       "\n",
       "                                        document_url answer_start  \\\n",
       "0  https://en.wikipedia.org/wiki/Quantum%20field%...        [159]   \n",
       "1  https://en.wikipedia.org/wiki/List%20of%20Nobe...        [610]   \n",
       "2            https://en.wikipedia.org/wiki/Dialectic        [129]   \n",
       "3  https://en.wikipedia.org/wiki/Origin%20of%20Ha...         [88]   \n",
       "4          https://en.wikipedia.org/wiki/Grasshopper          [0]   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0                                          ['1920s']   \n",
       "1                                ['Sully Prudhomme']   \n",
       "2  ['discourse between two or more people holding...   \n",
       "3                               ['Sejong the Great']   \n",
       "4  ['Grasshoppers are plant-eaters, with a few sp...   \n",
       "\n",
       "                             question_text_tokenized  \\\n",
       "0  ['when', 'was', 'quantum', 'field', 'theory', ...   \n",
       "1  ['who', 'was', 'the', 'first', 'nobel', 'prize...   \n",
       "2  ['when', 'is', 'the', 'dialectical', 'method',...   \n",
       "3                 ['who', 'invented', 'hangul', '?']   \n",
       "4         ['what', 'do', 'grasshoppers', 'eat', '?']   \n",
       "\n",
       "                        document_plaintext_tokenized  \\\n",
       "0  ['quantum', 'field', 'theory', 'naturally', 'b...   \n",
       "1  ['the', 'nobel', 'prize', 'in', 'literature', ...   \n",
       "2  ['dialectic', 'or', 'dialectics', '(', 'greek'...   \n",
       "3  ['hangul', 'was', 'personally', 'created', 'an...   \n",
       "4  ['grasshoppers', 'are', 'plant-eaters', ',', '...   \n",
       "\n",
       "                               answer_text_tokenized  labels  \n",
       "0                                          ['1920s']       1  \n",
       "1                             ['sully', 'prudhomme']       1  \n",
       "2  ['discourse', 'between', 'two', 'or', 'more', ...       1  \n",
       "3                         ['sejong', 'the', 'great']       1  \n",
       "4  ['grasshoppers', 'are', 'plant-eaters', ',', '...       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import training data\n",
    "dft_eng = pd.read_csv('../../data/dft_eng.csv')\n",
    "dft_jap = pd.read_csv('../../data/dft_jap.csv')\n",
    "dft_fin = pd.read_csv('../../data/dft_fin.csv')\n",
    "\n",
    "# import validation data\n",
    "dfv_eng = pd.read_csv('../../data/dfv_eng.csv')\n",
    "dfv_jap = pd.read_csv('../../data/dfv_jap.csv')\n",
    "dfv_fin = pd.read_csv('../../data/dfv_fin.csv')\n",
    "\n",
    "#import word count\n",
    "word_count = pd.read_csv('../../data/question_word_count.csv')\n",
    "\n",
    "dft_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english model with 25k word-pieces\n",
    "bpemb_en = BPEmb(lang='en', dim=100, vs=25000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpemb_features(dataset, bpemb):\n",
    "  # With bpemb we can tokenize and embed an entire document using .embed(x)\n",
    "  X = [bpemb.embed(x).mean(0) for x in (dataset.document_plaintext)]\n",
    "  y = dataset.labels\n",
    " \n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='multinomial')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,y_train = get_bpemb_features(dft_eng, bpemb_en)\n",
    "X_test,y_test = get_bpemb_features(dfv_eng, bpemb_en)\n",
    "lr_bpemb = LogisticRegression(penalty='l2', max_iter=1000, multi_class='multinomial')\n",
    "lr_bpemb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bpemb = lr_bpemb.predict(X_test)\n",
    "preds_valid_bpemb = lr_bpemb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708061</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.681342</td>\n",
       "      <td>495.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.679849</td>\n",
       "      <td>0.729293</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>495.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.693955</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692523</td>\n",
       "      <td>990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.693955</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692523</td>\n",
       "      <td>990.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.708061  0.656566  0.681342  495.000000\n",
       "1              0.679849  0.729293  0.703704  495.000000\n",
       "accuracy       0.692929  0.692929  0.692929    0.692929\n",
       "macro avg      0.693955  0.692929  0.692523  990.000000\n",
       "weighted avg   0.693955  0.692929  0.692523  990.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPEmb model \n",
    "report = classification_report(y_test, preds_bpemb, output_dict=True)\n",
    "pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text, model, tokenizer, max_length=100):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model(input_ids)\n",
    "    return output.hidden_states\n",
    "\n",
    "def generate_all(text, model, tokenizer, max_length, num_beams, no_reapeat_ngrams, temperature, top_k, top_p):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    greedy_output = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "    beam_search_output = model.generate(input_ids, max_length=max_length, do_sample=True, num_beams=num_beams)\n",
    "    n_grams_output = model.generate(input_ids, max_length=max_length, no_repeat_ngram_size=no_reapeat_ngrams, num_beams=num_beams)\n",
    "    sample_output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature)\n",
    "    top_k_otput = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=top_k)\n",
    "    top_p_output = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=top_k, top_p=top_p)\n",
    "\n",
    "    output_lst = [greedy_output, beam_search_output, n_grams_output, sample_output, top_k_otput, top_p_output]\n",
    "    decoded_samples = [tokenizer.decode(g[0], skip_special_tokens=True) for g in output_lst]\n",
    "\n",
    "    return decoded_samples\n",
    "\n",
    "def generate_top_p(text, model, tokenizer, max_length, top_p, top_k):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, do_sample=True,top_k=top_k, top_p=top_p)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_sample(text, model, tokenizer, max_length, temperature):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was quantum field theory developed?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The notion of a quantum field theory was first developed in 1947 with the creation of the first quantum field theory, but the following theories developed in the 1950s and 1960s and their publication, \"Quantum Field Theory in the Early Twentieth Century\" by John St. Louis, was a classic example of a very well understood theory.\n",
      "St. Louis, in a letter from the physicist David Schindler, said:\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "samples = generate_top_p(dft_eng.question_text[0], model, tokenizer, 100, 0.9, 50)\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0301,  0.3668,  0.0901,  ..., -0.2221,  0.1273, -0.1497],\n",
       "         [ 0.5815,  0.2135,  0.1955,  ...,  0.4937,  0.0897, -0.2369],\n",
       "         [ 0.2250,  0.5953, -0.3460,  ...,  0.3857, -0.0740,  0.1617]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate_top_p(\"The weather is\", model, tokenizer, 50, 0.95, 50)\n",
    "get_hidden_states(\"The weather is\", model, tokenizer)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('I was meaning to', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_perplexity(sentence, model, vocabulary, seq_len):\n",
    "  states = (torch.zeros(lstm_layers, 1, lstm_dim).to(device),\n",
    "              torch.zeros(lstm_layers, 1, lstm_dim).to(device))\n",
    "  token_ids = [{'input_ids': bpemb_en.encode_ids(sentence)}]\n",
    "  batch = collate_batch_bilstm(token_ids)\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "  logits, states = model(batch[0].to(device), batch[1].to(device), states)\n",
    "\n",
    "  target = batch[2].to(device)[:len(token_ids[0]['input_ids'])-1]\n",
    "  loss = loss_fn(logits, target.reshape(-1))\n",
    "  loss = loss.detach().cpu().numpy()\n",
    "  return np.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['wikitext-103-v1', 'wikitext-2-v1', 'wikitext-103-raw-v1', 'wikitext-2-raw-v1']\nExample of usage:\n\t`load_dataset('wikitext', 'wikitext-103-v1')`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mwikitext\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\datasets\\load.py:1670\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[0;32m   1669\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1670\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1671\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m   1672\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   1673\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1674\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1675\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1676\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1677\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   1678\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   1679\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1680\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1681\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1682\u001b[0m )\n\u001b[0;32m   1684\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\datasets\\load.py:1473\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m   1472\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 1473\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[0;32m   1474\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1475\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[0;32m   1476\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1477\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1478\u001b[0m     \u001b[39mhash\u001b[39m\u001b[39m=\u001b[39m\u001b[39mhash\u001b[39m,\n\u001b[0;32m   1479\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1480\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1481\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuilder_kwargs,\n\u001b[0;32m   1482\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1483\u001b[0m )\n\u001b[0;32m   1485\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\datasets\\builder.py:1285\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder.__init__\u001b[1;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, writer_batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 1285\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1286\u001b[0m     \u001b[39m# Batch size used by the ArrowWriter\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m     \u001b[39m# It defines the number of samples that are kept in memory before writing them\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     \u001b[39m# and also the length of the arrow chunks\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m     \u001b[39m# None means that the ArrowWriter will use its default value\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer_batch_size \u001b[39m=\u001b[39m writer_batch_size \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\datasets\\builder.py:303\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mif\u001b[39;00m data_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data_dir\n\u001b[1;32m--> 303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_builder_config(\n\u001b[0;32m    304\u001b[0m     config_name,\n\u001b[0;32m    305\u001b[0m     custom_features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    306\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m    307\u001b[0m )\n\u001b[0;32m    309\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\datasets\\builder.py:432\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[1;34m(self, name, custom_features, **config_kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBUILDER_CONFIGS) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    431\u001b[0m     example_of_usage \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mload_dataset(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBUILDER_CONFIGS[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    433\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConfig name is missing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPlease pick one among the available configs: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder_configs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mExample of usage:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00mexample_of_usage\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m builder_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBUILDER_CONFIGS[\u001b[39m0\u001b[39m]\n\u001b[0;32m    438\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo config specified, defaulting to the single config: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mbuilder_config\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['wikitext-103-v1', 'wikitext-2-v1', 'wikitext-103-raw-v1', 'wikitext-2-raw-v1']\nExample of usage:\n\t`load_dataset('wikitext', 'wikitext-103-v1')`"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/Hallgrimur/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\n\\n\".join(dft_eng[\"question_text\"])\n",
    "encodings\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/562 [00:00<-1:58:43, -7.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m target_ids[:, :\u001b[39m-\u001b[39mtrg_len] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids, labels\u001b[39m=\u001b[39;49mtarget_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# loss is calculated using CrossEntropyLoss which averages over input tokens.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Multiply it with trg_len to get the summation instead of average.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# We will take average over all the tokens to get the true average\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# in the last step of this example.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     neg_log_likelihood \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss \u001b[39m*\u001b[39m trg_len\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1078\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[39m# Flatten the tokens\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1078\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(shift_logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, shift_logits\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), shift_labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1081\u001b[0m     output \u001b[39m=\u001b[39m (lm_logits,) \u001b[39m+\u001b[39m transformer_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "        # Multiply it with trg_len to get the summation instead of average.\n",
    "        # We will take average over all the tokens to get the true average\n",
    "        # in the last step of this example.\n",
    "        neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(39.2577)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [24915, 388, 2214, 4583, 8752, 2540, 351, 262, 2050, 286, 31094, 12213, 11, 355, 262, 31094, 2214, 373, 262, 691, 1900, 15993, 2214, 355, 286, 262, 14062, 82, 3693, 23, 5974, 16], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dft_eng.document_plaintext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"document_plaintext\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    keys = ['attention_mask', 'input_ids']\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in keys}\n",
    "    total_length = len(concatenated_examples[list(keys)[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # this is needed as the used dataset is a subclass of ClassificationDataset, which requires label as a field...\n",
    "    result[\"label\"] = result[\"input_ids\"].copy()\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 'The previous mayor, Bill Laforet faced a recall election in November 2018, after a resident group submitted in June a list of 5,000 petition signatures that they had collected calling for the action, in excess of the 25% needed to place the measure in front of voters.[85] In the November 2018 general election, Laforet was recalled from office and John Roth was elected mayor. The successful recall was the first in the county for at least 25 years.[86]'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = {'train': dft_eng.document_plaintext[x] for x in range(len(dft_eng.document_plaintext))}\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m textlst \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(dft_eng\u001b[39m.\u001b[39mdocument_plaintext)), \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train \u001b[39m=\u001b[39m {textlst[x]: dft_eng\u001b[39m.\u001b[39mdocument_plaintext[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dft_eng\u001b[39m.\u001b[39mdocument_plaintext))}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train\n",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 26\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m textlst \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(dft_eng\u001b[39m.\u001b[39mdocument_plaintext)), \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train \u001b[39m=\u001b[39m {textlst[x]: dft_eng\u001b[39m.\u001b[39mdocument_plaintext[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dft_eng\u001b[39m.\u001b[39mdocument_plaintext))}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "textlst = np.full((1, len(dft_eng.document_plaintext)), 'text').tolist()\n",
    "train = {textlst[x]: dft_eng.document_plaintext[x] for x in range(len(dft_eng.document_plaintext))}\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokonizedt = [tokenizer(x) for x in dft_eng.question_text]\n",
    "tokonizedv = [tokenizer(x) for x in dfv_eng.question_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokonizedt,\n",
    "    eval_dataset=tokonizedv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 990\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 11 at dim 1 (got 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mevaluate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerplexity: \u001b[39m\u001b[39m{\u001b[39;00mmath\u001b[39m.\u001b[39mexp(eval_results[\u001b[39m'\u001b[39m\u001b[39meval_loss\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2281\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2283\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2284\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2285\u001b[0m     eval_dataloader,\n\u001b[0;32m   2286\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2287\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2288\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2289\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2290\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2291\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2292\u001b[0m )\n\u001b[0;32m   2294\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2295\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2296\u001b[0m     speed_metrics(\n\u001b[0;32m   2297\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     )\n\u001b[0;32m   2302\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2448\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2446\u001b[0m observed_num_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   2447\u001b[0m \u001b[39m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 2448\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m   2449\u001b[0m     \u001b[39m# Update the observed num examples\u001b[39;00m\n\u001b[0;32m   2450\u001b[0m     observed_batch_size \u001b[39m=\u001b[39m find_batch_size(inputs)\n\u001b[0;32m   2451\u001b[0m     \u001b[39mif\u001b[39;00m observed_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     67\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:130\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    128\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features])\n\u001b[0;32m    129\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[0;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 11 at dim 1 (got 10)"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tydiqa (C:/Users/Hallgrimur/.cache/huggingface/datasets/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67014aa117584c11bc947336beb03bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = load_dataset('tydiqa', 'primary_task')\n",
    "#unsupervised_imdb_splits = unsupervised_imdb.train_test_split(test_size=0.01)\n",
    "#print(unsupervised_imdb_splits.keys())\n",
    "#print(unsupervised_imdb_splits['train'][0])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage_answer_candidates</th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'plaintext_start_byte': [1, 660, 844, 1196, 1...</td>\n",
       "      <td>berapakah jenis ras yang ada didunia?</td>\n",
       "      <td>Ras manusia</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\ntransl.\\n\\nRas (dari bahasa Prancis race, ya...</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Ras%20manusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'plaintext_start_byte': [1, 271, 995, 1763, 2...</td>\n",
       "      <td>2018年アメリカで一番治安の悪い州はどこ</td>\n",
       "      <td>デトロイト</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n\\nデトロイト（ /dɨˈtrɔɪt/）は、アメリカ合衆国ミシガン州南東部にある都市...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'plaintext_start_byte': [0, 208, 542, 891, 10...</td>\n",
       "      <td>Je,Ngamia anaweza kaa bila maji kwa muda gani?</td>\n",
       "      <td>Kuku</td>\n",
       "      <td>swahili</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\nKuku (Gallus gallus domesticus) ni ndege ana...</td>\n",
       "      <td>https://sw.wikipedia.org/wiki/Kuku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'plaintext_start_byte': [5, 401, 1185, 2103, ...</td>\n",
       "      <td>কম্পিউটার বিজ্ঞানের মোট কয়টি শাখা রয়েছে ?</td>\n",
       "      <td>বিজ্ঞান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nভৌত বিশ্বের যা কিছু পর্যবেক্ষণ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'plaintext_start_byte': [0, 395, 716, 1320, 3...</td>\n",
       "      <td>మెదక్ నగర విస్తీర్ణం ఎంత?</td>\n",
       "      <td>మెదక్ జిల్లా</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>మెదక్ జిల్లా తెలంగాణ రాష్ట్రంలోని 31 జిల్లాలలో...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%AE%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>{'plaintext_start_byte': [1, 1142, 3214, 3713,...</td>\n",
       "      <td>మొట్టమొదటి కెమెరా పేరేమిటి ?</td>\n",
       "      <td>కెమెరా</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\nకెమెరా (ఆంగ్లం: Camera) అనగా స్థిర చిత్రాలను...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%95%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>{'plaintext_start_byte': [0, 249, 541, 1064, 1...</td>\n",
       "      <td>大和民族より前に日本列島に住んでいた民族はいる？</td>\n",
       "      <td>大和民族</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'passage_answer_candidate_index': [37], 'mini...</td>\n",
       "      <td>\\n\\n大和民族（やまとみんぞく）は、日本列島の住民の大半を占める民族である。ほとんどが日本...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E5%A4%A7%E5%92%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>{'plaintext_start_byte': [0, 378, 457, 1348, 1...</td>\n",
       "      <td>كم عدد آيات سورة الحديد؟</td>\n",
       "      <td>سورة الحديد</td>\n",
       "      <td>arabic</td>\n",
       "      <td>{'passage_answer_candidate_index': [0], 'minim...</td>\n",
       "      <td>\\nسورة الحديد  هي سورة مدنية عدد آياتها 29 وتر...</td>\n",
       "      <td>https://ar.wikipedia.org/wiki/%D8%B3%D9%88%D8%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>{'plaintext_start_byte': [0, 544, 675, 1140, 1...</td>\n",
       "      <td>Kuinka monta romaania Stephen King on kirjoitt...</td>\n",
       "      <td>Stephen King</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n\\nStephen Edwin King (s. 21. syyskuuta 194...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Stephen%20King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>{'plaintext_start_byte': [2, 510, 1094, 1489, ...</td>\n",
       "      <td>2018년에 가장 오래된 시계는 무엇인가?</td>\n",
       "      <td>시계</td>\n",
       "      <td>korean</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n시계(時計, )는 시간을 나타내거나 시간을 재는 기계나 장치이다. 시계에는 ...</td>\n",
       "      <td>https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B3%84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               passage_answer_candidates  \\\n",
       "0      {'plaintext_start_byte': [1, 660, 844, 1196, 1...   \n",
       "1      {'plaintext_start_byte': [1, 271, 995, 1763, 2...   \n",
       "2      {'plaintext_start_byte': [0, 208, 542, 891, 10...   \n",
       "3      {'plaintext_start_byte': [5, 401, 1185, 2103, ...   \n",
       "4      {'plaintext_start_byte': [0, 395, 716, 1320, 3...   \n",
       "...                                                  ...   \n",
       "99995  {'plaintext_start_byte': [1, 1142, 3214, 3713,...   \n",
       "99996  {'plaintext_start_byte': [0, 249, 541, 1064, 1...   \n",
       "99997  {'plaintext_start_byte': [0, 378, 457, 1348, 1...   \n",
       "99998  {'plaintext_start_byte': [0, 544, 675, 1140, 1...   \n",
       "99999  {'plaintext_start_byte': [2, 510, 1094, 1489, ...   \n",
       "\n",
       "                                           question_text document_title  \\\n",
       "0                  berapakah jenis ras yang ada didunia?    Ras manusia   \n",
       "1                                  2018年アメリカで一番治安の悪い州はどこ          デトロイト   \n",
       "2         Je,Ngamia anaweza kaa bila maji kwa muda gani?           Kuku   \n",
       "3              কম্পিউটার বিজ্ঞানের মোট কয়টি শাখা রয়েছে ?        বিজ্ঞান   \n",
       "4                              మెదక్ నగర విస్తీర్ణం ఎంత?   మెదక్ జిల్లా   \n",
       "...                                                  ...            ...   \n",
       "99995                       మొట్టమొదటి కెమెరా పేరేమిటి ?         కెమెరా   \n",
       "99996                           大和民族より前に日本列島に住んでいた民族はいる？           大和民族   \n",
       "99997                           كم عدد آيات سورة الحديد؟    سورة الحديد   \n",
       "99998  Kuinka monta romaania Stephen King on kirjoitt...   Stephen King   \n",
       "99999                            2018년에 가장 오래된 시계는 무엇인가?             시계   \n",
       "\n",
       "         language                                        annotations  \\\n",
       "0      indonesian  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "1        japanese  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "2         swahili  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "3         bengali  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "4          telugu  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "...           ...                                                ...   \n",
       "99995      telugu  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "99996    japanese  {'passage_answer_candidate_index': [37], 'mini...   \n",
       "99997      arabic  {'passage_answer_candidate_index': [0], 'minim...   \n",
       "99998     finnish  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "99999      korean  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "\n",
       "                                      document_plaintext  \\\n",
       "0      \\ntransl.\\n\\nRas (dari bahasa Prancis race, ya...   \n",
       "1      \\n\\n\\nデトロイト（ /dɨˈtrɔɪt/）は、アメリカ合衆国ミシガン州南東部にある都市...   \n",
       "2      \\nKuku (Gallus gallus domesticus) ni ndege ana...   \n",
       "3      \\n\\n\\n\\n\\n\\n\\n\\nভৌত বিশ্বের যা কিছু পর্যবেক্ষণ...   \n",
       "4      మెదక్ జిల్లా తెలంగాణ రాష్ట్రంలోని 31 జిల్లాలలో...   \n",
       "...                                                  ...   \n",
       "99995  \\nకెమెరా (ఆంగ్లం: Camera) అనగా స్థిర చిత్రాలను...   \n",
       "99996  \\n\\n大和民族（やまとみんぞく）は、日本列島の住民の大半を占める民族である。ほとんどが日本...   \n",
       "99997  \\nسورة الحديد  هي سورة مدنية عدد آياتها 29 وتر...   \n",
       "99998  \\n\\n\\nStephen Edwin King (s. 21. syyskuuta 194...   \n",
       "99999  \\n\\n시계(時計, )는 시간을 나타내거나 시간을 재는 기계나 장치이다. 시계에는 ...   \n",
       "\n",
       "                                            document_url  \n",
       "0            https://id.wikipedia.org/wiki/Ras%20manusia  \n",
       "1      https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%...  \n",
       "2                     https://sw.wikipedia.org/wiki/Kuku  \n",
       "3      https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%...  \n",
       "4      https://te.wikipedia.org/wiki/%E0%B0%AE%E0%B1%...  \n",
       "...                                                  ...  \n",
       "99995  https://te.wikipedia.org/wiki/%E0%B0%95%E0%B1%...  \n",
       "99996  https://ja.wikipedia.org/wiki/%E5%A4%A7%E5%92%...  \n",
       "99997  https://ar.wikipedia.org/wiki/%D8%B3%D9%88%D8%...  \n",
       "99998       https://fi.wikipedia.org/wiki/Stephen%20King  \n",
       "99999   https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B3%84  \n",
       "\n",
       "[100000 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame.from_dict(train['train'][0:100000]) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage_answer_candidates</th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'plaintext_start_byte': [2, 740, 1381, 1941, ...</td>\n",
       "      <td>When did the art deco movement begin?</td>\n",
       "      <td>Art Deco</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\nArt Deco, sometimes referred to as Deco, i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Art%20Deco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'plaintext_start_byte': [5, 378, 956, 1342, 3...</td>\n",
       "      <td>Is Creole a pidgin of French?</td>\n",
       "      <td>French-based creole languages</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [1], 'minim...</td>\n",
       "      <td>\\n\\n\\n\\n\\nPart of a series on theFrench langua...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/French-based%20c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>{'plaintext_start_byte': [7, 1637, 1938, 2380,...</td>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [12], 'mini...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nQuantum field theoryFeynman diag...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>{'plaintext_start_byte': [2, 284, 580, 837, 10...</td>\n",
       "      <td>What was the highest value of the yen in 2018?</td>\n",
       "      <td>Banknotes of the Japanese yen</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\nThe banknotes of the Japanese yen are part...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Banknotes%20of%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>{'plaintext_start_byte': [3, 118, 357, 1045, 1...</td>\n",
       "      <td>Does plastic decompose at all?</td>\n",
       "      <td>Biodegradable plastic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [0], 'minim...</td>\n",
       "      <td>\\n\\n\\nBiodegradable plastics are plastics that...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Biodegradable%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99899</th>\n",
       "      <td>{'plaintext_start_byte': [3, 517, 858, 1228, 1...</td>\n",
       "      <td>When was ultrasound first used in medicine?</td>\n",
       "      <td>Medical ultrasound</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [59], 'mini...</td>\n",
       "      <td>\\n\\n\\nMedical ultrasound (also known as diagno...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Medical%20ultras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99917</th>\n",
       "      <td>{'plaintext_start_byte': [0, 171, 498, 868, 15...</td>\n",
       "      <td>Do steam locomotives have gears?</td>\n",
       "      <td>Geared steam locomotive</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [0], 'minim...</td>\n",
       "      <td>A geared steam locomotive is a type of steam l...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Geared%20steam%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99942</th>\n",
       "      <td>{'plaintext_start_byte': [1, 459, 636, 1198, 1...</td>\n",
       "      <td>When was the West Virginia Mountaineers basket...</td>\n",
       "      <td>West Virginia Mountaineers men's basketball</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [4], 'minim...</td>\n",
       "      <td>\\nThe West Virginia Mountaineers men's basketb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/West%20Virginia%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99943</th>\n",
       "      <td>{'plaintext_start_byte': [3, 595, 1443, 2355, ...</td>\n",
       "      <td>How long does it take solar wind to reach the ...</td>\n",
       "      <td>Solar wind</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>\\n\\n\\nThe solar wind is a stream of charged pa...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Solar%20wind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99962</th>\n",
       "      <td>{'plaintext_start_byte': [0, 460, 1755, 3058, ...</td>\n",
       "      <td>How much does it cost to adopt a baby?</td>\n",
       "      <td>Adoption in California</td>\n",
       "      <td>english</td>\n",
       "      <td>{'passage_answer_candidate_index': [-1], 'mini...</td>\n",
       "      <td>More adoptions occur in California each year t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adoption%20in%20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5540 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               passage_answer_candidates  \\\n",
       "28     {'plaintext_start_byte': [2, 740, 1381, 1941, ...   \n",
       "50     {'plaintext_start_byte': [5, 378, 956, 1342, 3...   \n",
       "65     {'plaintext_start_byte': [7, 1637, 1938, 2380,...   \n",
       "76     {'plaintext_start_byte': [2, 284, 580, 837, 10...   \n",
       "87     {'plaintext_start_byte': [3, 118, 357, 1045, 1...   \n",
       "...                                                  ...   \n",
       "99899  {'plaintext_start_byte': [3, 517, 858, 1228, 1...   \n",
       "99917  {'plaintext_start_byte': [0, 171, 498, 868, 15...   \n",
       "99942  {'plaintext_start_byte': [1, 459, 636, 1198, 1...   \n",
       "99943  {'plaintext_start_byte': [3, 595, 1443, 2355, ...   \n",
       "99962  {'plaintext_start_byte': [0, 460, 1755, 3058, ...   \n",
       "\n",
       "                                           question_text  \\\n",
       "28                 When did the art deco movement begin?   \n",
       "50                         Is Creole a pidgin of French?   \n",
       "65              When was quantum field theory developed?   \n",
       "76        What was the highest value of the yen in 2018?   \n",
       "87                        Does plastic decompose at all?   \n",
       "...                                                  ...   \n",
       "99899        When was ultrasound first used in medicine?   \n",
       "99917                   Do steam locomotives have gears?   \n",
       "99942  When was the West Virginia Mountaineers basket...   \n",
       "99943  How long does it take solar wind to reach the ...   \n",
       "99962             How much does it cost to adopt a baby?   \n",
       "\n",
       "                                    document_title language  \\\n",
       "28                                        Art Deco  english   \n",
       "50                   French-based creole languages  english   \n",
       "65                            Quantum field theory  english   \n",
       "76                   Banknotes of the Japanese yen  english   \n",
       "87                           Biodegradable plastic  english   \n",
       "...                                            ...      ...   \n",
       "99899                           Medical ultrasound  english   \n",
       "99917                      Geared steam locomotive  english   \n",
       "99942  West Virginia Mountaineers men's basketball  english   \n",
       "99943                                   Solar wind  english   \n",
       "99962                       Adoption in California  english   \n",
       "\n",
       "                                             annotations  \\\n",
       "28     {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "50     {'passage_answer_candidate_index': [1], 'minim...   \n",
       "65     {'passage_answer_candidate_index': [12], 'mini...   \n",
       "76     {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "87     {'passage_answer_candidate_index': [0], 'minim...   \n",
       "...                                                  ...   \n",
       "99899  {'passage_answer_candidate_index': [59], 'mini...   \n",
       "99917  {'passage_answer_candidate_index': [0], 'minim...   \n",
       "99942  {'passage_answer_candidate_index': [4], 'minim...   \n",
       "99943  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "99962  {'passage_answer_candidate_index': [-1], 'mini...   \n",
       "\n",
       "                                      document_plaintext  \\\n",
       "28     \\n\\nArt Deco, sometimes referred to as Deco, i...   \n",
       "50     \\n\\n\\n\\n\\nPart of a series on theFrench langua...   \n",
       "65     \\n\\n\\n\\n\\n\\n\\nQuantum field theoryFeynman diag...   \n",
       "76     \\n\\nThe banknotes of the Japanese yen are part...   \n",
       "87     \\n\\n\\nBiodegradable plastics are plastics that...   \n",
       "...                                                  ...   \n",
       "99899  \\n\\n\\nMedical ultrasound (also known as diagno...   \n",
       "99917  A geared steam locomotive is a type of steam l...   \n",
       "99942  \\nThe West Virginia Mountaineers men's basketb...   \n",
       "99943  \\n\\n\\nThe solar wind is a stream of charged pa...   \n",
       "99962  More adoptions occur in California each year t...   \n",
       "\n",
       "                                            document_url  \n",
       "28              https://en.wikipedia.org/wiki/Art%20Deco  \n",
       "50     https://en.wikipedia.org/wiki/French-based%20c...  \n",
       "65     https://en.wikipedia.org/wiki/Quantum%20field%...  \n",
       "76     https://en.wikipedia.org/wiki/Banknotes%20of%2...  \n",
       "87     https://en.wikipedia.org/wiki/Biodegradable%20...  \n",
       "...                                                  ...  \n",
       "99899  https://en.wikipedia.org/wiki/Medical%20ultras...  \n",
       "99917  https://en.wikipedia.org/wiki/Geared%20steam%2...  \n",
       "99942  https://en.wikipedia.org/wiki/West%20Virginia%...  \n",
       "99943         https://en.wikipedia.org/wiki/Solar%20wind  \n",
       "99962  https://en.wikipedia.org/wiki/Adoption%20in%20...  \n",
       "\n",
       "[5540 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['language'] == 'english']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85ac075dbf107ca10f05bcc388d88f275addd2a735b233d21443c8ee15d0b537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
