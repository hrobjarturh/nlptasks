{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from bpemb import BPEmb\n",
    "from tqdm import tqdm\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>question_text_tokenized</th>\n",
       "      <th>document_plaintext_tokenized</th>\n",
       "      <th>answer_text_tokenized</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([159]), 'answer_text': ...</td>\n",
       "      <td>Quantum field theory naturally began with the ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "      <td>[159]</td>\n",
       "      <td>['1920s']</td>\n",
       "      <td>['when', 'was', 'quantum', 'field', 'theory', ...</td>\n",
       "      <td>['quantum', 'field', 'theory', 'naturally', 'b...</td>\n",
       "      <td>['1920s']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first Nobel prize winner for Liter...</td>\n",
       "      <td>List of Nobel laureates in Literature</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([610]), 'answer_text': ...</td>\n",
       "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Nobe...</td>\n",
       "      <td>[610]</td>\n",
       "      <td>['Sully Prudhomme']</td>\n",
       "      <td>['who', 'was', 'the', 'first', 'nobel', 'prize...</td>\n",
       "      <td>['the', 'nobel', 'prize', 'in', 'literature', ...</td>\n",
       "      <td>['sully', 'prudhomme']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When is the dialectical method used?</td>\n",
       "      <td>Dialectic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([129]), 'answer_text': ...</td>\n",
       "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dialectic</td>\n",
       "      <td>[129]</td>\n",
       "      <td>['discourse between two or more people holding...</td>\n",
       "      <td>['when', 'is', 'the', 'dialectical', 'method',...</td>\n",
       "      <td>['dialectic', 'or', 'dialectics', '(', 'greek'...</td>\n",
       "      <td>['discourse', 'between', 'two', 'or', 'more', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who invented Hangul?</td>\n",
       "      <td>Origin of Hangul</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([88]), 'answer_text': a...</td>\n",
       "      <td>Hangul was personally created and promulgated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Origin%20of%20Ha...</td>\n",
       "      <td>[88]</td>\n",
       "      <td>['Sejong the Great']</td>\n",
       "      <td>['who', 'invented', 'hangul', '?']</td>\n",
       "      <td>['hangul', 'was', 'personally', 'created', 'an...</td>\n",
       "      <td>['sejong', 'the', 'great']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do Grasshoppers eat?</td>\n",
       "      <td>Grasshopper</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': array([0]), 'answer_text': ar...</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grasshopper</td>\n",
       "      <td>[0]</td>\n",
       "      <td>['Grasshoppers are plant-eaters, with a few sp...</td>\n",
       "      <td>['what', 'do', 'grasshoppers', 'eat', '?']</td>\n",
       "      <td>['grasshoppers', 'are', 'plant-eaters', ',', '...</td>\n",
       "      <td>['grasshoppers', 'are', 'plant-eaters', ',', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question_text  \\\n",
       "0           When was quantum field theory developed?   \n",
       "1  Who was the first Nobel prize winner for Liter...   \n",
       "2               When is the dialectical method used?   \n",
       "3                               Who invented Hangul?   \n",
       "4                          What do Grasshoppers eat?   \n",
       "\n",
       "                          document_title language  \\\n",
       "0                   Quantum field theory  english   \n",
       "1  List of Nobel laureates in Literature  english   \n",
       "2                              Dialectic  english   \n",
       "3                       Origin of Hangul  english   \n",
       "4                            Grasshopper  english   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  {'answer_start': array([159]), 'answer_text': ...   \n",
       "1  {'answer_start': array([610]), 'answer_text': ...   \n",
       "2  {'answer_start': array([129]), 'answer_text': ...   \n",
       "3  {'answer_start': array([88]), 'answer_text': a...   \n",
       "4  {'answer_start': array([0]), 'answer_text': ar...   \n",
       "\n",
       "                                  document_plaintext  \\\n",
       "0  Quantum field theory naturally began with the ...   \n",
       "1  The Nobel Prize in Literature (Swedish: Nobelp...   \n",
       "2  Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
       "3  Hangul was personally created and promulgated ...   \n",
       "4  Grasshoppers are plant-eaters, with a few spec...   \n",
       "\n",
       "                                        document_url answer_start  \\\n",
       "0  https://en.wikipedia.org/wiki/Quantum%20field%...        [159]   \n",
       "1  https://en.wikipedia.org/wiki/List%20of%20Nobe...        [610]   \n",
       "2            https://en.wikipedia.org/wiki/Dialectic        [129]   \n",
       "3  https://en.wikipedia.org/wiki/Origin%20of%20Ha...         [88]   \n",
       "4          https://en.wikipedia.org/wiki/Grasshopper          [0]   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0                                          ['1920s']   \n",
       "1                                ['Sully Prudhomme']   \n",
       "2  ['discourse between two or more people holding...   \n",
       "3                               ['Sejong the Great']   \n",
       "4  ['Grasshoppers are plant-eaters, with a few sp...   \n",
       "\n",
       "                             question_text_tokenized  \\\n",
       "0  ['when', 'was', 'quantum', 'field', 'theory', ...   \n",
       "1  ['who', 'was', 'the', 'first', 'nobel', 'prize...   \n",
       "2  ['when', 'is', 'the', 'dialectical', 'method',...   \n",
       "3                 ['who', 'invented', 'hangul', '?']   \n",
       "4         ['what', 'do', 'grasshoppers', 'eat', '?']   \n",
       "\n",
       "                        document_plaintext_tokenized  \\\n",
       "0  ['quantum', 'field', 'theory', 'naturally', 'b...   \n",
       "1  ['the', 'nobel', 'prize', 'in', 'literature', ...   \n",
       "2  ['dialectic', 'or', 'dialectics', '(', 'greek'...   \n",
       "3  ['hangul', 'was', 'personally', 'created', 'an...   \n",
       "4  ['grasshoppers', 'are', 'plant-eaters', ',', '...   \n",
       "\n",
       "                               answer_text_tokenized  labels  \n",
       "0                                          ['1920s']       1  \n",
       "1                             ['sully', 'prudhomme']       1  \n",
       "2  ['discourse', 'between', 'two', 'or', 'more', ...       1  \n",
       "3                         ['sejong', 'the', 'great']       1  \n",
       "4  ['grasshoppers', 'are', 'plant-eaters', ',', '...       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import training data\n",
    "dft_eng = pd.read_csv('../../data/dft_eng.csv')\n",
    "dft_jap = pd.read_csv('../../data/dft_jap.csv')\n",
    "dft_fin = pd.read_csv('../../data/dft_fin.csv')\n",
    "\n",
    "# import validation data\n",
    "dfv_eng = pd.read_csv('../../data/dfv_eng.csv')\n",
    "dfv_jap = pd.read_csv('../../data/dfv_jap.csv')\n",
    "dfv_fin = pd.read_csv('../../data/dfv_fin.csv')\n",
    "\n",
    "#import word count\n",
    "word_count = pd.read_csv('../../data/question_word_count.csv')\n",
    "\n",
    "dft_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english model with 25k word-pieces\n",
    "bpemb_en = BPEmb(lang='en', dim=100, vs=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpemb_features(dataset, bpemb):\n",
    "  # With bpemb we can tokenize and embed an entire document using .embed(x)\n",
    "  X = [bpemb.embed(x).mean(0) for x in (dataset.document_plaintext)]\n",
    "  y = dataset.labels\n",
    " \n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='multinomial')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,y_train = get_bpemb_features(dft_eng, bpemb_en)\n",
    "X_test,y_test = get_bpemb_features(dfv_eng, bpemb_en)\n",
    "lr_bpemb = LogisticRegression(penalty='l2', max_iter=1000, multi_class='multinomial')\n",
    "lr_bpemb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bpemb = lr_bpemb.predict(X_test)\n",
    "preds_valid_bpemb = lr_bpemb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708061</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.681342</td>\n",
       "      <td>495.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.679849</td>\n",
       "      <td>0.729293</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>495.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.693955</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692523</td>\n",
       "      <td>990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.693955</td>\n",
       "      <td>0.692929</td>\n",
       "      <td>0.692523</td>\n",
       "      <td>990.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "0              0.708061  0.656566  0.681342  495.000000\n",
       "1              0.679849  0.729293  0.703704  495.000000\n",
       "accuracy       0.692929  0.692929  0.692929    0.692929\n",
       "macro avg      0.693955  0.692929  0.692523  990.000000\n",
       "weighted avg   0.693955  0.692929  0.692523  990.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPEmb model \n",
    "report = classification_report(y_test, preds_bpemb, output_dict=True)\n",
    "pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text, model, tokenizer, max_length=100):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model(input_ids)\n",
    "    return output.hidden_states\n",
    "\n",
    "def generate_all(text, model, tokenizer, max_length, num_beams, no_reapeat_ngrams, temperature, top_k, top_p):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    greedy_output = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "    beam_search_output = model.generate(input_ids, max_length=max_length, do_sample=True, num_beams=num_beams)\n",
    "    n_grams_output = model.generate(input_ids, max_length=max_length, no_repeat_ngram_size=no_reapeat_ngrams, num_beams=num_beams)\n",
    "    sample_output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature)\n",
    "    top_k_otput = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=top_k)\n",
    "    top_p_output = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=top_k, top_p=top_p)\n",
    "\n",
    "    output_lst = [greedy_output, beam_search_output, n_grams_output, sample_output, top_k_otput, top_p_output]\n",
    "    decoded_samples = [tokenizer.decode(g[0], skip_special_tokens=True) for g in output_lst]\n",
    "\n",
    "    return decoded_samples\n",
    "\n",
    "def generate_top_p(text, model, tokenizer, max_length, top_p, top_k):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, do_sample=True,top_k=top_k, top_p=top_p)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_sample(text, model, tokenizer, max_length, temperature):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=temperature)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was quantum field theory developed? Was it the effect of quantum field theory itself? How does quantum field theory explain the quantum field of quantum field theory?\n",
      "\n",
      "\n",
      "Here we are in a state of quantum gravity. A quantum field of quantum gravity exists which cannot be measured and not measured, as opposed to the particles in the waveform. Therefore there are a few advantages to quantum field theory - that is, they provide quantum field theory (and thus can be demonstrated in the quantum field of quantum\n"
     ]
    }
   ],
   "source": [
    "samples = generate_top_p(dft_eng.question_text[0], model, tokenizer, 100, 0.9, 50)\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0301,  0.3668,  0.0901,  ..., -0.2221,  0.1273, -0.1497],\n",
       "         [ 0.5815,  0.2135,  0.1955,  ...,  0.4937,  0.0897, -0.2369],\n",
       "         [ 0.2250,  0.5953, -0.3460,  ...,  0.3857, -0.0740,  0.1617]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate_top_p(\"The weather is\", model, tokenizer, 50, 0.95, 50)\n",
    "get_hidden_states(\"The weather is\", model, tokenizer)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('I was meaning to', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_perplexity(sentence, model, vocabulary, seq_len):\n",
    "  states = (torch.zeros(lstm_layers, 1, lstm_dim).to(device),\n",
    "              torch.zeros(lstm_layers, 1, lstm_dim).to(device))\n",
    "  token_ids = [{'input_ids': bpemb_en.encode_ids(sentence)}]\n",
    "  batch = collate_batch_bilstm(token_ids)\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "  logits, states = model(batch[0].to(device), batch[1].to(device), states)\n",
    "\n",
    "  target = batch[2].to(device)[:len(token_ids[0]['input_ids'])-1]\n",
    "  loss = loss_fn(logits, target.reshape(-1))\n",
    "  loss = loss.detach().cpu().numpy()\n",
    "  return np.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task3.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task3.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mwikitext\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ee526826404048873b28ef72d85d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275cc33e78f141089d305c21f2eff166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to C:/Users/Hallgrimur/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64801d0ff4ac4f25b3e06645eaa121ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765882e33cbf4aec9dc0fcaaa1bb7128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5704606a728b4214a21309bdfb0645da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95281368f4d84f048dff4c1aebc7ea5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to C:/Users/Hallgrimur/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\n\\n\".join(dft_eng[\"question_text\"])\n",
    "encodings\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 205/562 [11:42<20:01,  3.37s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
    "        # Multiply it with trg_len to get the summation instead of average.\n",
    "        # We will take average over all the tokens to get the true average\n",
    "        # in the last step of this example.\n",
    "        neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85ac075dbf107ca10f05bcc388d88f275addd2a735b233d21443c8ee15d0b537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
