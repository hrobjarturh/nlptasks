{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import math \n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    " device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (C:/Users/Hallgrimur/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd9e7eec069430190bbb1a11465525b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "        num_rows: 116067\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "        num_rows: 13325\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>อยุธยามีกี่อำเภอ ?</td>\n",
       "      <td>จังหวัดพระนครศรีอยุธยา</td>\n",
       "      <td>thai</td>\n",
       "      <td>{'answer_start': [42], 'answer_text': ['16']}</td>\n",
       "      <td>\\nปัจจุบันจังหวัดพระนครศรีอยุธยาประกอบด้วย 16 ...</td>\n",
       "      <td>https://th.wikipedia.org/wiki/%E0%B8%88%E0%B8%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>كم عدد مرات فوز الأوروغواي ببطولة كاس العالم ل...</td>\n",
       "      <td>قائمة نهائيات كأس العالم</td>\n",
       "      <td>arabic</td>\n",
       "      <td>{'answer_start': [394], 'answer_text': ['بطولت...</td>\n",
       "      <td>أقيمت البطولة 21 مرة، شارك في النهائيات 78 دول...</td>\n",
       "      <td>https://ar.wikipedia.org/wiki/%D9%82%D8%A7%D8%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siapakah yang menemuka benua Amerika ?</td>\n",
       "      <td>Kristoforus Kolumbus</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>{'answer_start': [193], 'answer_text': ['orang...</td>\n",
       "      <td>Kolumbus bukanlah orang pertama yang tiba di A...</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Kristoforus%20Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>化学兵器禁止条約はどこで採択された？</td>\n",
       "      <td>化学兵器禁止条約</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [11], 'answer_text': ['パリ']}</td>\n",
       "      <td>1993年1月13日にパリにおいて署名がなされ、1997年4月29日に発効した[1]。実効的...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E5%8C%96%E5%AD%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ใครเป็นผู้ก่อการ ใน เหตุโจมตีในนอร์เวย์ พ.ศ. 2...</td>\n",
       "      <td>เหตุโจมตีในนอร์เวย์ พ.ศ. 2554</td>\n",
       "      <td>thai</td>\n",
       "      <td>{'answer_start': [400], 'answer_text': ['แอนเด...</td>\n",
       "      <td>เหตุโจมตีครั้งที่สองนั้นเกิดขึ้นในราวสองชั่วโม...</td>\n",
       "      <td>https://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13320</th>\n",
       "      <td>월드컵이 처음 개최된 나라는 어디인가?</td>\n",
       "      <td>FIFA 월드컵</td>\n",
       "      <td>korean</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>브라질의 펠레는 FIFA 월드컵에 출전한 모든 선수들 중 최고로 월드컵 기록이 좋은...</td>\n",
       "      <td>https://ko.wikipedia.org/wiki/FIFA%20%EC%9B%94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13321</th>\n",
       "      <td>Berapakah berat Ikan pari manta yag terbesar?</td>\n",
       "      <td>Ikan pari manta</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Ciri khas manta adalah sepasang \"tanduk\" di de...</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Ikan%20pari%20manta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13322</th>\n",
       "      <td>Когда впервые начали применять компьютерную гр...</td>\n",
       "      <td>CGI (графика)</td>\n",
       "      <td>russian</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Созданием движущихся изображений занимается ко...</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/CGI%20%28%D0%B3%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13323</th>\n",
       "      <td>หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร เกิดเมื่อไหร่?</td>\n",
       "      <td>หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร</td>\n",
       "      <td>thai</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>ลำดับหม่อมราชวงศ์สุขุมพันธุ์ บริพัตร\\n\\nพระบาท...</td>\n",
       "      <td>https://th.wikipedia.org/wiki/%E0%B8%AB%E0%B8%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13324</th>\n",
       "      <td>ماهى أول مؤلفة لي ج. ك. رولينغ ؟</td>\n",
       "      <td>ج. ك. رولينغ</td>\n",
       "      <td>arabic</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>وبين الحين والآخر، تبدي رولينغ تناقضًا في أرآئ...</td>\n",
       "      <td>https://ar.wikipedia.org/wiki/%D8%AC.%20%D9%83...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13325 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question_text  \\\n",
       "0                                     อยุธยามีกี่อำเภอ ?   \n",
       "1      كم عدد مرات فوز الأوروغواي ببطولة كاس العالم ل...   \n",
       "2                 Siapakah yang menemuka benua Amerika ?   \n",
       "3                                     化学兵器禁止条約はどこで採択された？   \n",
       "4      ใครเป็นผู้ก่อการ ใน เหตุโจมตีในนอร์เวย์ พ.ศ. 2...   \n",
       "...                                                  ...   \n",
       "13320                              월드컵이 처음 개최된 나라는 어디인가?   \n",
       "13321      Berapakah berat Ikan pari manta yag terbesar?   \n",
       "13322  Когда впервые начали применять компьютерную гр...   \n",
       "13323     หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร เกิดเมื่อไหร่?   \n",
       "13324                   ماهى أول مؤلفة لي ج. ك. رولينغ ؟   \n",
       "\n",
       "                        document_title    language  \\\n",
       "0               จังหวัดพระนครศรีอยุธยา        thai   \n",
       "1             قائمة نهائيات كأس العالم      arabic   \n",
       "2                 Kristoforus Kolumbus  indonesian   \n",
       "3                             化学兵器禁止条約    japanese   \n",
       "4        เหตุโจมตีในนอร์เวย์ พ.ศ. 2554        thai   \n",
       "...                                ...         ...   \n",
       "13320                         FIFA 월드컵      korean   \n",
       "13321                  Ikan pari manta  indonesian   \n",
       "13322                    CGI (графика)     russian   \n",
       "13323  หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร        thai   \n",
       "13324                     ج. ك. رولينغ      arabic   \n",
       "\n",
       "                                             annotations  \\\n",
       "0          {'answer_start': [42], 'answer_text': ['16']}   \n",
       "1      {'answer_start': [394], 'answer_text': ['بطولت...   \n",
       "2      {'answer_start': [193], 'answer_text': ['orang...   \n",
       "3          {'answer_start': [11], 'answer_text': ['パリ']}   \n",
       "4      {'answer_start': [400], 'answer_text': ['แอนเด...   \n",
       "...                                                  ...   \n",
       "13320        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13321        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13322        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13323        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13324        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                      document_plaintext  \\\n",
       "0      \\nปัจจุบันจังหวัดพระนครศรีอยุธยาประกอบด้วย 16 ...   \n",
       "1      أقيمت البطولة 21 مرة، شارك في النهائيات 78 دول...   \n",
       "2      Kolumbus bukanlah orang pertama yang tiba di A...   \n",
       "3      1993年1月13日にパリにおいて署名がなされ、1997年4月29日に発効した[1]。実効的...   \n",
       "4      เหตุโจมตีครั้งที่สองนั้นเกิดขึ้นในราวสองชั่วโม...   \n",
       "...                                                  ...   \n",
       "13320  브라질의 펠레는 FIFA 월드컵에 출전한 모든 선수들 중 최고로 월드컵 기록이 좋은...   \n",
       "13321  Ciri khas manta adalah sepasang \"tanduk\" di de...   \n",
       "13322  Созданием движущихся изображений занимается ко...   \n",
       "13323  ลำดับหม่อมราชวงศ์สุขุมพันธุ์ บริพัตร\\n\\nพระบาท...   \n",
       "13324  وبين الحين والآخر، تبدي رولينغ تناقضًا في أرآئ...   \n",
       "\n",
       "                                            document_url  \n",
       "0      https://th.wikipedia.org/wiki/%E0%B8%88%E0%B8%...  \n",
       "1      https://ar.wikipedia.org/wiki/%D9%82%D8%A7%D8%...  \n",
       "2      https://id.wikipedia.org/wiki/Kristoforus%20Ko...  \n",
       "3      https://ja.wikipedia.org/wiki/%E5%8C%96%E5%AD%...  \n",
       "4      https://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%...  \n",
       "...                                                  ...  \n",
       "13320  https://ko.wikipedia.org/wiki/FIFA%20%EC%9B%94...  \n",
       "13321  https://id.wikipedia.org/wiki/Ikan%20pari%20manta  \n",
       "13322  https://ru.wikipedia.org/wiki/CGI%20%28%D0%B3%...  \n",
       "13323  https://th.wikipedia.org/wiki/%E0%B8%AB%E0%B8%...  \n",
       "13324  https://ar.wikipedia.org/wiki/%D8%AC.%20%D9%83...  \n",
       "\n",
       "[13325 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data \n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "train_set = train_set.to_pandas()\n",
    "validation_set = validation_set.to_pandas()\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>化学兵器禁止条約はどこで採択された？</td>\n",
       "      <td>化学兵器禁止条約</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [11], 'answer_text': ['パリ']}</td>\n",
       "      <td>1993年1月13日にパリにおいて署名がなされ、1997年4月29日に発効した[1]。実効的...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E5%8C%96%E5%AD%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>オリヴィア・デ・ハヴィランドが生まれたのはいつ</td>\n",
       "      <td>オリヴィア・デ・ハヴィランド</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [46], 'answer_text': ['1916年7...</td>\n",
       "      <td>\\nオリヴィア・デ・ハヴィランド（Dame Olivia De Havilland, DBE...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kauanko lasia on valmistettu?</td>\n",
       "      <td>Lasi</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [160], 'answer_text': ['noin ...</td>\n",
       "      <td>Vanhin tunnettu lasilaatu on alkali­kalkki­las...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Lasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mikä on Ponzi-huijaus?</td>\n",
       "      <td>Ponzi-huijaus</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [39], 'answer_text': ['pyrami...</td>\n",
       "      <td>Ponzi-huijaus eli Ponzi-järjestelmä on pyramid...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Ponzi-huijaus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mikä oli Napoleonin sotien lopputulos?</td>\n",
       "      <td>Napoleonin sodat</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [15], 'answer_text': ['Ranska...</td>\n",
       "      <td>Sotien jälkeen Ranska alistettiin kovilla rauh...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Napoleonin%20sodat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>What is the most common first word by babies?</td>\n",
       "      <td>Vocabulary development</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Social pragmatic theories, also in contrast to...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vocabulary%20dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>Kuinka kauan valtiopäivät kestää?</td>\n",
       "      <td>Suomen valtiopäivät</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>\\nEnnen vuotta 1906 kokoontuivat säätyvaltiopä...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Suomen%20valtiop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13315</th>\n",
       "      <td>アイスランド共和国の首都はどこですか？</td>\n",
       "      <td>アイスランド</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>こうした危機を乗り切るため、アイスランド中央銀行は8日にロシアから40億ユーロの緊急融資を受...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%82%A2%E3%82%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>Milloin käytiin Persianlahden sota?</td>\n",
       "      <td>Persianlahden sota</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Ilmaiskuilla oli erityisen tuhoisa vaikutus Ir...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Persianlahden%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13319</th>\n",
       "      <td>When did the Bundaberg Central State School be...</td>\n",
       "      <td>Bundaberg Central State School</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>By the 1880s the school site had become valuab...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bundaberg%20Cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3712 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question_text  \\\n",
       "3                                     化学兵器禁止条約はどこで採択された？   \n",
       "9                                オリヴィア・デ・ハヴィランドが生まれたのはいつ   \n",
       "11                         Kauanko lasia on valmistettu?   \n",
       "14                                Mikä on Ponzi-huijaus?   \n",
       "20                Mikä oli Napoleonin sotien lopputulos?   \n",
       "...                                                  ...   \n",
       "13305      What is the most common first word by babies?   \n",
       "13307                  Kuinka kauan valtiopäivät kestää?   \n",
       "13315                                アイスランド共和国の首都はどこですか？   \n",
       "13317                Milloin käytiin Persianlahden sota?   \n",
       "13319  When did the Bundaberg Central State School be...   \n",
       "\n",
       "                       document_title  language  \\\n",
       "3                            化学兵器禁止条約  japanese   \n",
       "9                      オリヴィア・デ・ハヴィランド  japanese   \n",
       "11                               Lasi   finnish   \n",
       "14                      Ponzi-huijaus   finnish   \n",
       "20                   Napoleonin sodat   finnish   \n",
       "...                               ...       ...   \n",
       "13305          Vocabulary development   english   \n",
       "13307             Suomen valtiopäivät   finnish   \n",
       "13315                          アイスランド  japanese   \n",
       "13317              Persianlahden sota   finnish   \n",
       "13319  Bundaberg Central State School   english   \n",
       "\n",
       "                                             annotations  \\\n",
       "3          {'answer_start': [11], 'answer_text': ['パリ']}   \n",
       "9      {'answer_start': [46], 'answer_text': ['1916年7...   \n",
       "11     {'answer_start': [160], 'answer_text': ['noin ...   \n",
       "14     {'answer_start': [39], 'answer_text': ['pyrami...   \n",
       "20     {'answer_start': [15], 'answer_text': ['Ranska...   \n",
       "...                                                  ...   \n",
       "13305        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13307        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13315        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13317        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13319        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                      document_plaintext  \\\n",
       "3      1993年1月13日にパリにおいて署名がなされ、1997年4月29日に発効した[1]。実効的...   \n",
       "9      \\nオリヴィア・デ・ハヴィランド（Dame Olivia De Havilland, DBE...   \n",
       "11     Vanhin tunnettu lasilaatu on alkali­kalkki­las...   \n",
       "14     Ponzi-huijaus eli Ponzi-järjestelmä on pyramid...   \n",
       "20     Sotien jälkeen Ranska alistettiin kovilla rauh...   \n",
       "...                                                  ...   \n",
       "13305  Social pragmatic theories, also in contrast to...   \n",
       "13307  \\nEnnen vuotta 1906 kokoontuivat säätyvaltiopä...   \n",
       "13315  こうした危機を乗り切るため、アイスランド中央銀行は8日にロシアから40億ユーロの緊急融資を受...   \n",
       "13317  Ilmaiskuilla oli erityisen tuhoisa vaikutus Ir...   \n",
       "13319  By the 1880s the school site had become valuab...   \n",
       "\n",
       "                                            document_url  \n",
       "3      https://ja.wikipedia.org/wiki/%E5%8C%96%E5%AD%...  \n",
       "9      https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%...  \n",
       "11                    https://fi.wikipedia.org/wiki/Lasi  \n",
       "14           https://fi.wikipedia.org/wiki/Ponzi-huijaus  \n",
       "20      https://fi.wikipedia.org/wiki/Napoleonin%20sodat  \n",
       "...                                                  ...  \n",
       "13305  https://en.wikipedia.org/wiki/Vocabulary%20dev...  \n",
       "13307  https://fi.wikipedia.org/wiki/Suomen%20valtiop...  \n",
       "13315  https://ja.wikipedia.org/wiki/%E3%82%A2%E3%82%...  \n",
       "13317  https://fi.wikipedia.org/wiki/Persianlahden%20...  \n",
       "13319  https://en.wikipedia.org/wiki/Bundaberg%20Cent...  \n",
       "\n",
       "[3712 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_language = ['english', 'japanese', 'finnish']\n",
    "\n",
    "train_set = train_set[train_set['language'].isin(chosen_language)]\n",
    "validation_set = validation_set[validation_set['language'].isin(chosen_language)]\n",
    "\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hallgrimur\\AppData\\Local\\Temp\\ipykernel_13932\\2290379095.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation_set['answerable'] = get_answerability(validation_set['annotations'].tolist())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['question_text', 'document_title', 'language', 'annotations',\n",
       "       'document_plaintext', 'document_url', 'answerable'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_answerability(annotations):\n",
    "    answerable = []\n",
    "    for annot in annotations:\n",
    "        if -1 in annot['answer_start']:\n",
    "            answerable.append(0)\n",
    "        else:\n",
    "            answerable.append(1)\n",
    "    return answerable\n",
    "\n",
    "train_set['answerable'] = get_answerability(train_set['annotations'].tolist())\n",
    "validation_set['answerable'] = get_answerability(validation_set['annotations'].tolist())\n",
    "\n",
    "train_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 29868\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 3712\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "def question_parag_combine(questions, paragraphs):\n",
    "    \"\"\"\n",
    "    This function combines the questions and paragraphs into a single text\n",
    "    Args:\n",
    "        questions: list of questions\n",
    "        paragraphs: list of paragraphs\n",
    "    Returns:\n",
    "        list of combined questions and paragraphs\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    for index in range(len(questions)):\n",
    "        training_data += [questions[index] + \"\\n\" + paragraphs[index]]\n",
    "        \n",
    "    return training_data\n",
    "\n",
    "def make_df(train_set, validation_set):\n",
    "\n",
    "  d_q = train_set['question_text'].tolist()\n",
    "  d_p = train_set['document_plaintext'].tolist()\n",
    "  training_data = question_parag_combine(d_q,d_p)\n",
    "  training_labels = train_set['answerable'].tolist()\n",
    "\n",
    "  d_q = validation_set['question_text'].tolist()\n",
    "  d_p = validation_set['document_plaintext'].tolist()\n",
    "  validation_data = question_parag_combine(d_q,d_p)\n",
    "  validation_labels = validation_set['answerable'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "  data_set = {}\n",
    "  sets = [['train',training_data, training_labels], ['val', validation_data, validation_labels]]\n",
    "  for meta in sets:\n",
    "      data_set[meta[0]] = {}\n",
    "      data_set[meta[0]]['text'] = []\n",
    "      data_set[meta[0]]['labels'] = []\n",
    "      \n",
    "      for ind, text in enumerate(meta[1]):\n",
    "          data_set[meta[0]]['text'].append(text)\n",
    "          data_set[meta[0]]['labels'].append(meta[2][ind])\n",
    "\n",
    "          \n",
    "\n",
    "          \n",
    "  data_set = DatasetDict({'train':Dataset.from_dict(data_set['train']),\n",
    "                          'valid':Dataset.from_dict(data_set['val'])\\\n",
    "                        })\n",
    "  return data_set\n",
    "\n",
    "data_set = make_df(train_set, validation_set)\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    keys = ['attention_mask', 'input_ids', 'token_type_ids']\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in keys}\n",
    "    total_length = len(concatenated_examples[list(keys)[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # this is needed as the used dataset is a subclass of ClassificationDataset, which requires label as a field...\n",
    "    result[\"label\"] = result[\"input_ids\"].copy()\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Hallgrimur/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at C:\\Users\\Hallgrimur/.cache\\huggingface\\transformers\\eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at C:\\Users\\Hallgrimur/.cache\\huggingface\\transformers\\46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Hallgrimur/.cache\\huggingface\\transformers\\f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Hallgrimur/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Hallgrimur/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Hallgrimur/.cache\\huggingface\\transformers\\0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertLMHeadModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, T5Tokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "#define multilingual model\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "#tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61556c47fcdb4ecfbdb1ced93be31fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d36fa9a50d492f9801c18368cdf85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize data\n",
    "tokenized_datasets = data_set.map(tokenize_function, batched=True, num_proc=1, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66838afa82024c7da26f4bc29c4a1ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86505b2313742648f3435f5cf70088d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets_lm = tokenized_datasets.map(group_texts, batched=True, batch_size=512, num_proc=1)\n",
    "tokenized_datasets_lm = tokenized_datasets_lm.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 119472\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 14848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "#training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    #max_steps=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_lm[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_lm[\"valid\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 14848\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209dc6e8b92648648efb7847fa2c48d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hallgrimur\\Desktop\\KU\\NLP\\nlptasks\\notebooks\\hali-temp\\task6.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hallgrimur/Desktop/KU/NLP/nlptasks/notebooks/hali-temp/task6.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2281\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2283\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2284\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2285\u001b[0m     eval_dataloader,\n\u001b[0;32m   2286\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2287\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2288\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2289\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2290\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2291\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2292\u001b[0m )\n\u001b[0;32m   2294\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2295\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2296\u001b[0m     speed_metrics(\n\u001b[0;32m   2297\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     )\n\u001b[0;32m   2302\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2458\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2455\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[0;32m   2457\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 2458\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[0;32m   2460\u001b[0m \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   2461\u001b[0m     xm\u001b[39m.\u001b[39mmark_step()\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2671\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   2669\u001b[0m \u001b[39mif\u001b[39;00m has_labels:\n\u001b[0;32m   2670\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[1;32m-> 2671\u001b[0m         loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   2672\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m   2674\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2043\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2041\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2042\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2043\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2044\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2045\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2046\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1226\u001b[0m, in \u001b[0;36mBertLMHeadModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1223\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1224\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1226\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1227\u001b[0m     input_ids,\n\u001b[0;32m   1228\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1229\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1230\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1231\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1232\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1233\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1234\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1235\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1236\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1237\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1238\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1239\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1240\u001b[0m )\n\u001b[0;32m   1242\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1243\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    987\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    989\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    990\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    991\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    995\u001b[0m )\n\u001b[1;32m--> 996\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    997\u001b[0m     embedding_output,\n\u001b[0;32m    998\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    999\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1000\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1001\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1002\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1003\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1004\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1005\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1006\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1007\u001b[0m )\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    576\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    578\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    586\u001b[0m         hidden_states,\n\u001b[0;32m    587\u001b[0m         attention_mask,\n\u001b[0;32m    588\u001b[0m         layer_head_mask,\n\u001b[0;32m    589\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    590\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    591\u001b[0m         past_key_value,\n\u001b[0;32m    592\u001b[0m         output_attentions,\n\u001b[0;32m    593\u001b[0m     )\n\u001b[0;32m    595\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    510\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    511\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 513\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    514\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    515\u001b[0m )\n\u001b[0;32m    516\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    518\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2928\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   2925\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m   2926\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m-> 2928\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:526\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m    525\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 526\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[0;32m    527\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:441\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    439\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    440\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m--> 441\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLayerNorm(hidden_states \u001b[39m+\u001b[39;49m input_tensor)\n\u001b[0;32m    442\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[0;32m    190\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[1;32mc:\\Users\\Hallgrimur\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2503\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2499\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   2500\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2501\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[0;32m   2502\u001b[0m     )\n\u001b[1;32m-> 2503\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85ac075dbf107ca10f05bcc388d88f275addd2a735b233d21443c8ee15d0b537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
