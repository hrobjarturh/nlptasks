{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 - Sequence labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2\n",
      "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/knud/anaconda3/lib/python3.8/site-packages (from fasttext) (65.4.1)\n",
      "Requirement already satisfied: numpy in /home/knud/anaconda3/lib/python3.8/site-packages (from fasttext) (1.21.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=4697318 sha256=c6e96ddb82a5c35c92a5bb11ff6747a5cfcb9b7ab811410073b8a3b347a06216\n",
      "  Stored in directory: /home/knud/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.10.0\n",
      "Collecting pytorch-crf\n",
      "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n",
      "Requirement already satisfied: datasets in /home/knud/anaconda3/lib/python3.8/site-packages (2.5.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (2022.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (0.10.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: pandas in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: xxhash in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: packaging in /home/knud/.local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: multiprocess in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: aiohttp in /home/knud/anaconda3/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/knud/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/knud/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: filelock in /home/knud/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/knud/.local/lib/python3.8/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/knud/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/knud/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/knud/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/knud/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/knud/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/knud/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/knud/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/knud/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/knud/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/knud/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/knud/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/knud/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/knud/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/knud/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext\n",
    "!pip install pytorch-crf\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:59:51.729931: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:59:51.908614: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-14 11:59:51.908641: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-14 11:59:51.946086: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-14 11:59:52.879084: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-14 11:59:52.879182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-14 11:59:52.879205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-10-14 11:59:53.775974: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-14 11:59:53.775995: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-14 11:59:53.776034: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gadelampe-S403FA): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "# from torchcrf import CRF\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
    "from typing import List, Tuple, AnyStr\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "import heapq\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../../stat-nlp-book/\")\n",
    "\n",
    "import statnlpbook.util as util\n",
    "import statnlpbook.sequence as seq\n",
    "from statnlpbook.gmb import load_gmb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently \n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (/home/gadelampe/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2e97d99b2a43bc9992d0358703503c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = datasets.data['validation'].to_pandas()\n",
    "dft = datasets.data['train'].to_pandas()\n",
    "\n",
    "dfv = dfv.loc[dfv['language'].isin(['english', 'japanese','finnish'])].reset_index(drop=True)\n",
    "dft = dft.loc[dft['language'].isin(['english', 'japanese','finnish'])].reset_index(drop=True)\n",
    "\n",
    "dfv_eng = dfv.loc[dfv['language']=='english'].reset_index(drop=True)\n",
    "# dfv_jap = dfv.loc[dfv['language']=='japanese'].reset_index(drop=True)\n",
    "# dfv_fin = dfv.loc[dfv['language']=='finnish'].reset_index(drop=True)\n",
    "\n",
    "dft_eng = dft.loc[dft['language']=='english'].reset_index(drop=True)\n",
    "# dft_jap = dft.loc[dft['language']=='japanese'].reset_index(drop=True)\n",
    "# dft_fin = dft.loc[dft['language']=='finnish'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "dft_eng = pd.read_csv('../../data/dft_eng.csv')\n",
    "dft_jap = pd.read_csv('../../data/dft_jap.csv')\n",
    "dft_fin = pd.read_csv('../../data/dft_fin.csv')\n",
    "\n",
    "# import validation data\n",
    "dfv_eng = pd.read_csv('../../data/dfv_eng.csv')\n",
    "dfv_jap = pd.read_csv('../../data/dfv_jap.csv')\n",
    "dfv_fin = pd.read_csv('../../data/dfv_fin.csv')\n",
    "\n",
    "#import word count\n",
    "word_count = pd.read_csv('../../data/question_word_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m dft_eng[\u001b[39m'\u001b[39m\u001b[39mquestion_text_tokenized\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [punct\u001b[39m.\u001b[39mtokenize(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dft_eng\u001b[39m.\u001b[39mquestion_text\u001b[39m.\u001b[39mvalues]\n\u001b[1;32m     12\u001b[0m \u001b[39m# dft_jap['question_text_tokenized'] = [wakati.parse(i).split() for i in dft_jap.question_text.values]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# dft_fin['question_text_tokenized'] = [tbwt.tokenize(i) for i in dft_fin.question_text.values]\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dfv_eng[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [punct\u001b[39m.\u001b[39mtokenize(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dfv_eng\u001b[39m.\u001b[39mdocument_plaintext\u001b[39m.\u001b[39mvalues]\n\u001b[1;32m     16\u001b[0m \u001b[39m# dfv_jap['document_plaintext_tokenized'] = [wakati.parse(i).split() for i in dfv_jap.document_plaintext.values]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# dfv_fin['document_plaintext_tokenized'] = [tbwt.tokenize(i) for i in dfv_fin.document_plaintext.values]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m dft_eng[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [punct\u001b[39m.\u001b[39mtokenize(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dft_eng\u001b[39m.\u001b[39mdocument_plaintext\u001b[39m.\u001b[39mvalues]\n",
      "Cell \u001b[0;32mIn [27], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m dft_eng[\u001b[39m'\u001b[39m\u001b[39mquestion_text_tokenized\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [punct\u001b[39m.\u001b[39mtokenize(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dft_eng\u001b[39m.\u001b[39mquestion_text\u001b[39m.\u001b[39mvalues]\n\u001b[1;32m     12\u001b[0m \u001b[39m# dft_jap['question_text_tokenized'] = [wakati.parse(i).split() for i in dft_jap.question_text.values]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# dft_fin['question_text_tokenized'] = [tbwt.tokenize(i) for i in dft_fin.question_text.values]\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dfv_eng[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [punct\u001b[39m.\u001b[39;49mtokenize(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dfv_eng\u001b[39m.\u001b[39mdocument_plaintext\u001b[39m.\u001b[39mvalues]\n\u001b[1;32m     16\u001b[0m \u001b[39m# dfv_jap['document_plaintext_tokenized'] = [wakati.parse(i).split() for i in dfv_jap.document_plaintext.values]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# dfv_fin['document_plaintext_tokenized'] = [tbwt.tokenize(i) for i in dfv_fin.document_plaintext.values]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m dft_eng[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [punct\u001b[39m.\u001b[39mtokenize(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dft_eng\u001b[39m.\u001b[39mdocument_plaintext\u001b[39m.\u001b[39mvalues]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tokenize/regexp.py:133\u001b[0m, in \u001b[0;36mRegexpTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_regexp\u001b[39m.\u001b[39msplit(text)\n\u001b[1;32m    131\u001b[0m \u001b[39m# If our regexp matches tokens, use re.findall:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_regexp\u001b[39m.\u001b[39;49mfindall(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# tbwt = TreebankWordTokenizer()\n",
    "# stan = StanfordTokenizer()\n",
    "# word = word_tokenize()\n",
    "punct = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "dfv_eng['question_text_tokenized'] = [punct.tokenize(i) for i in dfv_eng.question_text.values]\n",
    "# dfv_jap['question_text_tokenized'] = [wakati.parse(i).split() for i in dfv_jap.question_text.values]\n",
    "# dfv_fin['question_text_tokenized'] = [tbwt.tokenize(i) for i in dfv_fin.question_text.values]\n",
    "\n",
    "dft_eng['question_text_tokenized'] = [punct.tokenize(i) for i in dft_eng.question_text.values]\n",
    "# dft_jap['question_text_tokenized'] = [wakati.parse(i).split() for i in dft_jap.question_text.values]\n",
    "# dft_fin['question_text_tokenized'] = [tbwt.tokenize(i) for i in dft_fin.question_text.values]\n",
    "\n",
    "dfv_eng['document_plaintext_tokenized'] = [punct.tokenize(i) for i in dfv_eng.document_plaintext.values]\n",
    "# dfv_jap['document_plaintext_tokenized'] = [wakati.parse(i).split() for i in dfv_jap.document_plaintext.values]\n",
    "# dfv_fin['document_plaintext_tokenized'] = [tbwt.tokenize(i) for i in dfv_fin.document_plaintext.values]\n",
    "\n",
    "dft_eng['document_plaintext_tokenized'] = [punct.tokenize(i) for i in dft_eng.document_plaintext.values]\n",
    "# dft_jap['document_plaintext_tokenized'] = [wakati.parse(i).split() for i in dft_jap.document_plaintext.values]\n",
    "# dft_fin['document_plaintext_tokenized'] = [tbwt.tokenize(i) for i in dft_fin.document_plaintext.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_text                   Who was the first Nobel prize winner for Liter...\n",
       "document_title                              List of Nobel laureates in Literature\n",
       "language                                                                  english\n",
       "annotations                     {'answer_start': [610], 'answer_text': ['Sully...\n",
       "document_plaintext              The Nobel Prize in Literature (Swedish: Nobelp...\n",
       "document_url                    https://en.wikipedia.org/wiki/List%20of%20Nobe...\n",
       "question_text_tokenized         [Who, was, the, first, Nobel, prize, winner, f...\n",
       "document_plaintext_tokenized    [The, Nobel, Prize, in, Literature, (, Swedish...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_eng.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Quantum, field, theory, naturally, began, wit...\n",
       "1       [The, Nobel, Prize, in, Literature, (, Swedish...\n",
       "2       [Dialectic, or, dialectics, (, Greek, :, διαλε...\n",
       "3       [Hangul, was, personally, created, and, promul...\n",
       "4       [Grasshoppers, are, plant, -, eaters, ,, with,...\n",
       "                              ...                        \n",
       "7384    [The, medley, relay, was, scheduled, in, the, ...\n",
       "7385    [Sāmkhya, is, a, dualist, philosophical, tradi...\n",
       "7386    [Mollo, was, surprised, by, the, success, of, ...\n",
       "7387    [In, the, end, ,, President, Truman, made, the...\n",
       "7388    [The, previous, mayor, ,, Bill, Laforet, faced...\n",
       "Name: document_plaintext_tokenized, Length: 7389, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_tokens = dft_eng['document_plaintext_tokenized']\n",
    "\n",
    "d_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They</td>\n",
       "      <td>marched</td>\n",
       "      <td>from</td>\n",
       "      <td>the</td>\n",
       "      <td>Houses</td>\n",
       "      <td>of</td>\n",
       "      <td>Parliament</td>\n",
       "      <td>to</td>\n",
       "      <td>a</td>\n",
       "      <td>rally</td>\n",
       "      <td>in</td>\n",
       "      <td>Hyde</td>\n",
       "      <td>Park</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRP</td>\n",
       "      <td>VBD</td>\n",
       "      <td>IN</td>\n",
       "      <td>DT</td>\n",
       "      <td>NNS</td>\n",
       "      <td>IN</td>\n",
       "      <td>NN</td>\n",
       "      <td>TO</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>IN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NNP</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1     2    3       4   5           6   7   8      9   10    11  \\\n",
       "0  They  marched  from  the  Houses  of  Parliament  to   a  rally  in  Hyde   \n",
       "1   PRP      VBD    IN   DT     NNS  IN          NN  TO  DT     NN  IN   NNP   \n",
       "\n",
       "     12 13  \n",
       "0  Park  .  \n",
       "1   NNP  .  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, pos, ents = load_gmb_dataset('../../../../stat-nlp-book/data/gmb/data/GMB_dataset_utf8.txt')\n",
    "\n",
    "pd.DataFrame([tokens[2], pos[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_1(x,i):\n",
    "    return {\n",
    "        'bias': 1.0,\n",
    "        'word:' + x[i]: 1.0,\n",
    "    }\n",
    "\n",
    "train = list(zip(tokens[:-200], pos[:-200]))\n",
    "dev = list(zip(tokens[-200:], pos[-200:]))\n",
    "\n",
    "local_1 = seq.LocalSequenceLabeler(feat_1, train, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Families',\n",
       " 'of',\n",
       " 'soldiers',\n",
       " 'killed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'conflict',\n",
       " 'joined',\n",
       " 'the',\n",
       " 'protesters',\n",
       " 'who',\n",
       " 'carried',\n",
       " 'banners',\n",
       " 'with',\n",
       " 'such',\n",
       " 'slogans',\n",
       " 'as',\n",
       " '\"',\n",
       " 'Bush',\n",
       " 'Number',\n",
       " 'One',\n",
       " 'Terrorist',\n",
       " '\"',\n",
       " 'and',\n",
       " '\"',\n",
       " 'Stop',\n",
       " 'the',\n",
       " 'Bombings',\n",
       " '.',\n",
       " '\"']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Nobel Prize in Literature (Swedish: Nobelpriset i litteratur) is awarded annually by the Swedish Academy to authors for outstanding contributions in the field of literature. It is one of the five Nobel Prizes established by the 1895 will of Alfred Nobel, which are awarded for outstanding contributions in chemistry, physics, literature, peace, and physiology or medicine.[1] As dictated by Nobel's will, the award is administered by the Nobel Foundation and awarded by a committee that consists of five members elected by the Swedish Academy.[2] The first Nobel Prize in Literature was awarded in 1901 to Sully Prudhomme of France.[3] Each recipient receives a medal, a diploma and a monetary award prize that has varied throughout the years.[4] In 1901, Prudhomme received 150,782 SEK, which is equivalent to 8,823,637.78 SEK in January 2018. The award is presented in Stockholm at an annual ceremony on December 10, the anniversary of Nobel's death.[5]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nlp(dft_eng['document_plaintext'][1])\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Nobel Prize in Literature -- WORK_OF_ART -- Titles of books, songs, etc.\n",
      "Swedish -- NORP -- Nationalities or religious or political groups\n",
      "annually -- DATE -- Absolute or relative dates or periods\n",
      "the Swedish Academy -- ORG -- Companies, agencies, institutions, etc.\n",
      "one -- CARDINAL -- Numerals that do not fall under another type\n",
      "Nobel Prizes -- WORK_OF_ART -- Titles of books, songs, etc.\n",
      "1895 -- DATE -- Absolute or relative dates or periods\n",
      "Alfred Nobel -- PERSON -- People, including fictional\n",
      "Nobel's -- WORK_OF_ART -- Titles of books, songs, etc.\n",
      "the Nobel Foundation -- ORG -- Companies, agencies, institutions, etc.\n",
      "five -- CARDINAL -- Numerals that do not fall under another type\n",
      "Swedish -- NORP -- Nationalities or religious or political groups\n",
      "first -- ORDINAL -- \"first\", \"second\", etc.\n",
      "Nobel Prize in Literature -- WORK_OF_ART -- Titles of books, songs, etc.\n",
      "1901 -- DATE -- Absolute or relative dates or periods\n",
      "1901 -- DATE -- Absolute or relative dates or periods\n",
      "Prudhomme -- ORG -- Companies, agencies, institutions, etc.\n",
      "150,782 -- CARDINAL -- Numerals that do not fall under another type\n",
      "SEK -- ORG -- Companies, agencies, institutions, etc.\n",
      "8,823,637.78 -- CARDINAL -- Numerals that do not fall under another type\n",
      "SEK -- GPE -- Countries, cities, states\n",
      "January 2018 -- DATE -- Absolute or relative dates or periods\n",
      "Stockholm -- GPE -- Countries, cities, states\n",
      "annual -- DATE -- Absolute or relative dates or periods\n",
      "December 10 -- DATE -- Absolute or relative dates or periods\n",
      "Nobel's death.[5] -- WORK_OF_ART -- Titles of books, songs, etc.\n"
     ]
    }
   ],
   "source": [
    "for ent in test.ents:\n",
    "    print(ent.text + ' -- ' + ent.label_ + ' -- ' + spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'answer_start': array([129]), 'answer_text': array(['discourse between two or more people holding different points of view about a subject but wishing to establish the truth through reasoned arguments'],\\n      dtype=object)}\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_eng['annotations'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'answer_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'answer_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(dft_eng[\u001b[39m'\u001b[39m\u001b[39manswer_text\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m4\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(dft_eng[\u001b[39m'\u001b[39m\u001b[39manswer_text\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mstrip(\u001b[39m'\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m\\'\u001b[39;00m\u001b[39m[\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'answer_text'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(dft_eng['answer_text'][4])\n",
    "print(dft_eng['answer_text'][4].strip(']\\'\\'[').split(' '))\n",
    "# print(re.split(r' |,|.', dft_eng['answer_text'][4].strip(']\\'\\'[')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_eng['answer_text_tokenized'] = [i.strip(']\\'\\'[').split(' ') for i in dft_eng.answer_text.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                 [1920s]\n",
       "1                                      [Sully, Prudhomme]\n",
       "2       [discourse, between, two, or, more, people, ho...\n",
       "3                                    [Sejong, the, Great]\n",
       "4       [Grasshoppers, are, plant-eaters,, with, a, fe...\n",
       "                              ...                        \n",
       "7384                                                   []\n",
       "7385                                                   []\n",
       "7386                                                   []\n",
       "7387                                                   []\n",
       "7388                                                   []\n",
       "Name: answer_text_tokenized, Length: 7389, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_eng['answer_text_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = dft_eng['document_plaintext_tokenized'][0]\n",
    "# B = dft_eng['answer_text_tokenized'][0]\n",
    "# IOB = []\n",
    "# for i,x in enumerate(A):\n",
    "#     if x in B and (i-1) >= 0 and IOB[i-1]=='O' or x in B and i==0:\n",
    "#         IOB.append('B')\n",
    "#     elif x in B and (i-1) >= 0 and IOB[i-1]=='B' or x in B and (i-1) >= 0 and IOB[i-1]=='I':\n",
    "#         IOB.append('I')\n",
    "#     else:\n",
    "#         IOB.append('O')\n",
    "\n",
    "# IOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOB = [[] for _ in range(dft_eng.shape[0])]\n",
    "A = dft_eng['document_plaintext_tokenized']\n",
    "B = dft_eng['answer_text_tokenized']\n",
    "\n",
    "for i, elm in enumerate(A):\n",
    "    for j, x in enumerate(elm):\n",
    "        if x in B[i] and (j-1) >= 0 and IOB[i][j-1]=='O' or x in B[i] and j==0:\n",
    "            IOB[i].append('B')\n",
    "        elif x in B[i] and (j-1) >= 0 and IOB[i][j-1]=='B' or x in B[i] and (j-1) >= 0 and IOB[i][j-1]=='I':\n",
    "            IOB[i].append('I')\n",
    "        else:\n",
    "            IOB[i].append('O')\n",
    "\n",
    "dft_eng['IOB'] = IOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Grasshoppers are plant-eaters, with a few species at times becoming serious pests of cereals, vegetables and pasture, especially when they swarm in their millions as locusts and destroy crops over wide areas']\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_eng['answer_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'I',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_eng['IOB'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'answer_start': array([0]), 'answer_text': array(['Grasshoppers are plant-eaters, with a few species at times becoming serious pests of cereals, vegetables and pasture, especially when they swarm in their millions as locusts and destroy crops over wide areas'],\\n      dtype=object)}\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_eng['annotations'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_text                                           What do Grasshoppers eat?\n",
       "document_title                                                        Grasshopper\n",
       "language                                                                  english\n",
       "annotations                     {'answer_start': array([0]), 'answer_text': ar...\n",
       "document_plaintext              Grasshoppers are plant-eaters, with a few spec...\n",
       "document_url                            https://en.wikipedia.org/wiki/Grasshopper\n",
       "answer_start                                                                  [0]\n",
       "answer_text                     ['Grasshoppers are plant-eaters, with a few sp...\n",
       "question_text_tokenized                          [What, do, Grasshoppers, eat, ?]\n",
       "document_plaintext_tokenized    [Grasshoppers, are, plant, -, eaters, ,, with,...\n",
       "answer_text_tokenized           [Grasshoppers, are, plant-eaters,, with, a, fe...\n",
       "labels                                                                          1\n",
       "IOB                             [B, I, O, O, O, O, B, I, I, I, I, I, I, I, I, ...\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_eng.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 16:48:02.651412: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:103\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:487\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    484\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[1;32m    485\u001b[0m         \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[0;32m--> 487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a `TypeSpec` for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    488\u001b[0m     element,\n\u001b[1;32m    489\u001b[0m     \u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a `TypeSpec` for 0       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n1       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n2       [O, B, O, O, O, O, O, O, O, O, O, B, O, O, O, ...\n3       [O, O, O, O, O, O, O, B, O, O, O, B, O, O, O, ...\n4       [B, I, O, O, O, O, B, I, I, I, I, I, I, I, I, ...\n                              ...                        \n7384    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n7385    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n7386    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n7387    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n7388    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\nName: IOB, Length: 7389, dtype: object with type Series",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m new_df_val \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(\u001b[39mdict\u001b[39m(dft_eng[[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIOB\u001b[39m\u001b[39m'\u001b[39m]]))\n\u001b[1;32m      4\u001b[0m new_df_val\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:814\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    737\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    738\u001b[0m   \u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \n\u001b[1;32m    740\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:4708\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m   4706\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, is_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   4707\u001b[0m   \u001b[39m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4708\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[1;32m   4709\u001b[0m   batched_spec \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m   4710\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:108\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m   normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 108\u001b[0m       ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i))\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, sparse_tensor\u001b[39m.\u001b[39mSparseTensorSpec):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1629\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1631\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1635\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1637\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1638\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1640\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1641\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "new_df_val = tf.data.Dataset.from_tensor_slices(dict(dft_eng[['document_plaintext_tokenized', 'IOB']]))\n",
    "new_df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-13 16:26:21--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650,22M  15,3MB/s    in 45s     \n",
      "\n",
      "2022-10-13 16:27:07 (14,5 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n",
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce down to our vocabulary and word embeddings\n",
    "def load_vectors(fname, vocabulary):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    tag_names = dft_eng.features[f\"IOB\"].feature.names\n",
    "    final_vocab = tag_names + ['[PAD]', '[UNK]', '[BOS]', '[EOS]']\n",
    "    final_vectors = [np.random.normal(size=(300,)) for _ in range(len(final_vocab))]\n",
    "    for j,line in enumerate(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in vocabulary or len(final_vocab) < 30000:\n",
    "          final_vocab.append(tokens[0])\n",
    "          final_vectors.append(np.array(list(map(float, tokens[1:]))))\n",
    "    return final_vocab, np.vstack(final_vectors)\n",
    "\n",
    "class FasttextTokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = {}\n",
    "        for j,l in enumerate(vocabulary):\n",
    "            self.vocab[l.strip()] = j\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Text is assumed to be tokenized\n",
    "        return [self.vocab[t] if t in self.vocab else self.vocab['[UNK]'] for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocabulary \u001b[39m=\u001b[39m (\u001b[39mset\u001b[39m([t \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m dft_eng \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m s[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m]]) \u001b[39m|\u001b[39m \u001b[39mset\u001b[39m([t \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m dfv_eng \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m s[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m]]))\n\u001b[1;32m      2\u001b[0m vocabulary, pretrained_embeddings \u001b[39m=\u001b[39m load_vectors(\u001b[39m'\u001b[39m\u001b[39mwiki-news-300d-1M.vec\u001b[39m\u001b[39m'\u001b[39m, vocabulary)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msize of vocabulary: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(vocabulary))\n",
      "Cell \u001b[0;32mIn [109], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocabulary \u001b[39m=\u001b[39m (\u001b[39mset\u001b[39m([t \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m dft_eng \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m s[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m]]) \u001b[39m|\u001b[39m \u001b[39mset\u001b[39m([t \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m dfv_eng \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m s[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext_tokenized\u001b[39m\u001b[39m'\u001b[39m]]))\n\u001b[1;32m      2\u001b[0m vocabulary, pretrained_embeddings \u001b[39m=\u001b[39m load_vectors(\u001b[39m'\u001b[39m\u001b[39mwiki-news-300d-1M.vec\u001b[39m\u001b[39m'\u001b[39m, vocabulary)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msize of vocabulary: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(vocabulary))\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "vocabulary = (set([t for s in dft_eng for t in s['document_plaintext_tokenized']]) | set([t for s in dfv_eng for t in s['document_plaintext_tokenized']]))\n",
    "vocabulary, pretrained_embeddings = load_vectors('wiki-news-300d-1M.vec', vocabulary)\n",
    "print('size of vocabulary: ', len(vocabulary))\n",
    "tokenizer = FasttextTokenizer(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
