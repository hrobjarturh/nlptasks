{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "dft_eng = pd.read_csv('../data/dft_eng.csv')\n",
    "\n",
    "# import validation data\n",
    "dfv_eng = pd.read_csv('../data/dfv_eng.csv')\n",
    "\n",
    "#import word count\n",
    "word_count = pd.read_csv('../data/question_word_count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create BoW vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, countt):\n",
    "    vec = np.zeros(len(countt))\n",
    "    for word in sentence:\n",
    "        vec[countt[word]] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_comb = [eval(que) + eval(doc) for que, doc in zip(dft_eng[\"question_text_tokenized\"],dft_eng[\"document_plaintext_tokenized\"])]\n",
    "v_comb = [eval(que) + eval(doc) for que, doc in zip(dfv_eng[\"question_text_tokenized\"],dfv_eng[\"document_plaintext_tokenized\"])]\n",
    "\n",
    "\n",
    "flat_tcomb = np.array([item for sublist in t_comb for item in sublist])\n",
    "flat_vcomb = np.array([item for sublist in v_comb for item in sublist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "countt_comb = Counter(flat_tcomb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bow vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_t = [make_bow_vector(comb, countt_comb) for comb in t_comb]\n",
    "bow_v = [make_bow_vector(comb, countt_comb) for comb in v_comb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add word frequency to bow vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_freq(que, doc):\n",
    "\n",
    "    freq = [x for x in que if x in doc]\n",
    "    freq = len(freq)/len(que)\n",
    "    return freq\n",
    "\n",
    "def freq_words_in_text (df_t,df_v):\n",
    "    for df in [df_t,df_v]:\n",
    "        frequency = []\n",
    "        for question, answer in zip(df['question_text_tokenized'], df['document_plaintext_tokenized']):\n",
    "            frequency.append(ans_freq(eval(question), eval(answer)))\n",
    "\n",
    "        df['word_frequency_score'] = frequency\n",
    "\n",
    "freq_words_in_text(dft_eng,dfv_eng)\n",
    "\n",
    "for index in range(len(bow_t)):\n",
    "    bow_t[index] = np.append(dft_eng['word_frequency_score'][index],bow_t[index])\n",
    "\n",
    "for index in range(len(bow_v)):\n",
    "    bow_v[index] = np.append(dfv_eng['word_frequency_score'][index],bow_v[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(bow_t)\n",
    "X_val = np.array(bow_v)\n",
    "y_train = dft_eng.labels.values\n",
    "y_val = dfv_eng.labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise logres model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1000, penalty='l1', random_state=1, solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7626262626262627"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_val)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_eng.to_csv('../data/dft_eng_word.csv',index=False) \n",
    "dfv_eng.to_csv('../data/dfv_eng_word.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'logres_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "962a7dd07f2ed9e1b57b96ee9c6defc8592f1a66c3054bde378394e0c30efb85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
