{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "dft_eng = pd.read_csv('../../data/dft_eng.csv')\n",
    "\n",
    "# import validation data\n",
    "dfv_eng = pd.read_csv('../../data/dfv_eng.csv')\n",
    "\n",
    "#import word count\n",
    "word_count = pd.read_csv('../../data/question_word_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['\\n','!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','a','about','above','after','again','against','all','am','an','and','any','are','as','at','be','because','been','before','being','below','between','both','but','by','can','did','do','does','doing','don','down','during','each','few','for','from','further','had','has','have','having','he','her','here','hers','herself','him','himself','his','how','i','if','in','into','is','it','its','itself','just','me','more','most','my','myself','no','nor','not','now','of','off','on','once','only','or','other','our','ours','ourselves','out','over','own','s','same','she','should','so','some','such','t','than','that','the','their','theirs','them','themselves','then','there','these','they','this','those','through','to','too','under','until','up','very','was','we','were','what','when','where','which','while','who','whom','why','will','with','you','your','yours','yourself','yourselves','{','|','}','~']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text_tokenized</th>\n",
       "      <th>document_plaintext_tokenized</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['when', 'was', 'quantum', 'field', 'theory', ...</td>\n",
       "      <td>['quantum', 'field', 'theory', 'naturally', 'b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['who', 'was', 'the', 'first', 'nobel', 'prize...</td>\n",
       "      <td>['the', 'nobel', 'prize', 'in', 'literature', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['when', 'is', 'the', 'dialectical', 'method',...</td>\n",
       "      <td>['dialectic', 'or', 'dialectics', '(', 'greek'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['who', 'invented', 'hangul', '?']</td>\n",
       "      <td>['hangul', 'was', 'personally', 'created', 'an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['what', 'do', 'grasshoppers', 'eat', '?']</td>\n",
       "      <td>['grasshoppers', 'are', 'plant-eaters', ',', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>['what', 'was', 'neil', 'brooks', \"'\", 'fastes...</td>\n",
       "      <td>['the', 'medley', 'relay', 'was', 'scheduled',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>['who', 'are', 'the', 'three', 'most', 'import...</td>\n",
       "      <td>['sāmkhya', 'is', 'a', 'dualist', 'philosophic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>['who', 'was', 'costume', 'designer', 'for', '...</td>\n",
       "      <td>['mollo', 'was', 'surprised', 'by', 'the', 'su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>['who', 'developed', 'the', 'first', 'thermonu...</td>\n",
       "      <td>['in', 'the', 'end', ',', 'president', 'truman...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>['what', 'is', 'the', 'population', 'of', 'mah...</td>\n",
       "      <td>['the', 'previous', 'mayor', ',', 'bill', 'laf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                question_text_tokenized  \\\n",
       "0     ['when', 'was', 'quantum', 'field', 'theory', ...   \n",
       "1     ['who', 'was', 'the', 'first', 'nobel', 'prize...   \n",
       "2     ['when', 'is', 'the', 'dialectical', 'method',...   \n",
       "3                    ['who', 'invented', 'hangul', '?']   \n",
       "4            ['what', 'do', 'grasshoppers', 'eat', '?']   \n",
       "...                                                 ...   \n",
       "7384  ['what', 'was', 'neil', 'brooks', \"'\", 'fastes...   \n",
       "7385  ['who', 'are', 'the', 'three', 'most', 'import...   \n",
       "7386  ['who', 'was', 'costume', 'designer', 'for', '...   \n",
       "7387  ['who', 'developed', 'the', 'first', 'thermonu...   \n",
       "7388  ['what', 'is', 'the', 'population', 'of', 'mah...   \n",
       "\n",
       "                           document_plaintext_tokenized  labels  \n",
       "0     ['quantum', 'field', 'theory', 'naturally', 'b...       1  \n",
       "1     ['the', 'nobel', 'prize', 'in', 'literature', ...       1  \n",
       "2     ['dialectic', 'or', 'dialectics', '(', 'greek'...       1  \n",
       "3     ['hangul', 'was', 'personally', 'created', 'an...       1  \n",
       "4     ['grasshoppers', 'are', 'plant-eaters', ',', '...       1  \n",
       "...                                                 ...     ...  \n",
       "7384  ['the', 'medley', 'relay', 'was', 'scheduled',...       0  \n",
       "7385  ['sāmkhya', 'is', 'a', 'dualist', 'philosophic...       0  \n",
       "7386  ['mollo', 'was', 'surprised', 'by', 'the', 'su...       0  \n",
       "7387  ['in', 'the', 'end', ',', 'president', 'truman...       0  \n",
       "7388  ['the', 'previous', 'mayor', ',', 'bill', 'laf...       0  \n",
       "\n",
       "[7389 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_df = dft_eng[[\"question_text_tokenized\", \"document_plaintext_tokenized\", \"labels\"]]\n",
    "\n",
    "bag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdoc_data = [eval(x) for x in dft_eng[\"document_plaintext_tokenized\"]]\n",
    "vdoc_data = [eval(x) for x in dfv_eng[\"document_plaintext_tokenized\"]]\n",
    "\n",
    "tque_data = [eval(x) for x in dft_eng[\"question_text_tokenized\"]]\n",
    "vque_data = [eval(x) for x in dfv_eng[\"question_text_tokenized\"]]\n",
    "\n",
    "t_comb = [eval(que) + eval(doc) for que, doc in zip(dft_eng[\"question_text_tokenized\"],dft_eng[\"document_plaintext_tokenized\"])]\n",
    "v_comb = [eval(que) + eval(doc) for que, doc in zip(dfv_eng[\"question_text_tokenized\"],dfv_eng[\"document_plaintext_tokenized\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdoc_data = [x for x in tdoc_data if not x in stop_words]\n",
    "vdoc_data = [eval(x) for x in dfv_eng[\"document_plaintext_tokenized\"]]\n",
    "\n",
    "tque_data = [eval(x) for x in dft_eng[\"question_text_tokenized\"]]\n",
    "vque_data = [eval(x) for x in dfv_eng[\"question_text_tokenized\"]]\n",
    "\n",
    "t_comb = [eval(que) + eval(doc) for que, doc in zip(dft_eng[\"question_text_tokenized\"],dft_eng[\"document_plaintext_tokenized\"])]\n",
    "v_comb = [eval(que) + eval(doc) for que, doc in zip(dfv_eng[\"question_text_tokenized\"],dfv_eng[\"document_plaintext_tokenized\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "doc_data_nostop = []\n",
    "for doc in tdoc_data:\n",
    "    tdoc_data.append([word for word in doc if word not in stop_words ])\n",
    "\n",
    "vdoc_data_nostop = []\n",
    "for doc in tdoc_data:\n",
    "    vdoc_data_nostop.vdoc_data([word for word in doc if word not in stop_words ])\n",
    "\n",
    "tque_data_nostop = []\n",
    "for doc in tque_data:\n",
    "    tque_data.append([word for word in doc if word not in stop_words ])\n",
    "\n",
    "vque_data_nostop = []\n",
    "for doc in vque_data:\n",
    "    vdoc_data_nostop.vdoc_data([word for word in doc if word not in stop_words ])\n",
    "\n",
    "'''\n",
    "t_comb_nostop = []\n",
    "for doc in t_comb:\n",
    "    t_comb_nostop.append([word for word in doc if word not in stop_words ])\n",
    "\n",
    "v_comb_nostop = []\n",
    "for doc in v_comb:\n",
    "    v_comb_nostop.append([word for word in doc if word not in stop_words ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_tdoc = np.array([item for sublist in tdoc_data for item in sublist])\n",
    "flat_vdoc = np.array([item for sublist in vdoc_data for item in sublist])\n",
    "\n",
    "flat_tque = np.array([item for sublist in tque_data for item in sublist])\n",
    "flat_vque = np.array([item for sublist in vdoc_data for item in sublist])\n",
    "\n",
    "flat_tcomb = np.array([item for sublist in t_comb for item in sublist])\n",
    "flat_vcomb = np.array([item for sublist in v_comb for item in sublist])\n",
    "\n",
    "flat_tcomb_nostop = np.array([item for sublist in t_comb_nostop for item in sublist])\n",
    "flat_vcomb_nostop = np.array([item for sublist in v_comb_nostop for item in sublist])\n",
    "\n",
    "countt_doc = Counter(flat_tdoc)\n",
    "countt_que = Counter(flat_tque)\n",
    "countt_comb = Counter(flat_tcomb)\n",
    "countt_comb_nostop = Counter(flat_vcomb_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create BoW vectors\n",
    "def make_bow_vector(sentence):\n",
    "    vec = np.zeros(len(countt_comb))\n",
    "    for word in sentence:\n",
    "        vec[countt_comb[word]] += 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [make_bow_vector(comb) for comb in t_comb_nostop]\n",
    "v1 = [make_bow_vector(comb) for comb in v_comb_nostop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [make_bow_vector(comb) for comb in t_comb]\n",
    "l2 = [make_bow_vector(que) for que in tque_data]\n",
    "l3 = [make_bow_vector(doc) for doc in tdoc_data]\n",
    "#np.array([make_bow_vector(que, countt_comb) for x in v_comb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = [make_bow_vector(comb) for comb in v_comb]\n",
    "v2 = [make_bow_vector(que) for que in vque_data]\n",
    "v3 = [make_bow_vector(doc) for doc in vdoc_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(l1)\n",
    "#X_train = np.array([np.concatenate([que,doc,comb]) for que, doc, comb in zip(l2,l3,l1)])\n",
    "#X_train = np.array([np.concatenate([que,doc]) for que, doc in zip(l2,l3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.array(v1)\n",
    "#X_val = np.array([np.concatenate([que, doc, comb]) for que, doc, comb in zip(v2,v3,v1)])\n",
    "#X_val = np.array([np.concatenate([que,doc]) for que, doc in zip(v2,v3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.array([np.concatenate([make_bow_vector(que, countt_comb) + make_bow_vector(doc, countt_comb) + make_bow_vector(comb, countt_comb)]) for que, doc, comb in zip(flat_tque, flat_tdoc, flat_tcomb)])\n",
    "#X_val = np.array([np.concatenate([make_bow_vector(que, countt_comb) + make_bow_vector(doc, countt_comb) + make_bow_vector(comb, countt_comb)]) for que, doc, comb in zip(flat_vque, flat_vdoc, flat_vcomb)])\n",
    "#X_val = np.array([make_bow_vector(x, countt_comb) for x in v_comb])\n",
    "\n",
    "y_train = dft_eng.labels.values\n",
    "y_val = dfv_eng.labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7389, 65320)\n",
      "(7389,)\n",
      "(990, 65320)\n",
      "(990,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7141414141414142"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1000, penalty='l1', random_state=1, solver='liblinear').fit(X_train, y_train)\n",
    "pred = clf.predict(X_val)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "962a7dd07f2ed9e1b57b96ee9c6defc8592f1a66c3054bde378394e0c30efb85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
