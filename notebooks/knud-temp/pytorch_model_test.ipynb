{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "dft_eng = pd.read_csv('../../data/dft_eng.csv')\n",
    "dft_jap = pd.read_csv('../../data/dft_jap.csv')\n",
    "dft_fin = pd.read_csv('../../data/dft_fin.csv')\n",
    "\n",
    "# import validation data\n",
    "dfv_eng = pd.read_csv('../../data/dfv_eng.csv')\n",
    "dfv_jap = pd.read_csv('../../data/dfv_jap.csv')\n",
    "dfv_fin = pd.read_csv('../../data/dfv_fin.csv')\n",
    "\n",
    "#import word count\n",
    "word_count = pd.read_csv('../../data/question_word_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['\\n','!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','a','about','above','after','again','against','all','am','an','and','any','are','as','at','be','because','been','before','being','below','between','both','but','by','can','did','do','does','doing','don','down','during','each','few','for','from','further','had','has','have','having','he','her','here','hers','herself','him','himself','his','how','i','if','in','into','is','it','its','itself','just','me','more','most','my','myself','no','nor','not','now','of','off','on','once','only','or','other','our','ours','ourselves','out','over','own','s','same','she','should','so','some','such','t','than','that','the','their','theirs','them','themselves','then','there','these','they','this','those','through','to','too','under','until','up','very','was','we','were','what','when','where','which','while','who','whom','why','will','with','you','your','yours','yourself','yourselves','{','|','}','~']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text_tokenized</th>\n",
       "      <th>document_plaintext_tokenized</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['when', 'was', 'quantum', 'field', 'theory', ...</td>\n",
       "      <td>['quantum', 'field', 'theory', 'naturally', 'b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['who', 'was', 'the', 'first', 'nobel', 'prize...</td>\n",
       "      <td>['the', 'nobel', 'prize', 'in', 'literature', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['when', 'is', 'the', 'dialectical', 'method',...</td>\n",
       "      <td>['dialectic', 'or', 'dialectics', '(', 'greek'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['who', 'invented', 'hangul', '?']</td>\n",
       "      <td>['hangul', 'was', 'personally', 'created', 'an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['what', 'do', 'grasshoppers', 'eat', '?']</td>\n",
       "      <td>['grasshoppers', 'are', 'plant-eaters', ',', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>['what', 'was', 'neil', 'brooks', \"'\", 'fastes...</td>\n",
       "      <td>['the', 'medley', 'relay', 'was', 'scheduled',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>['who', 'are', 'the', 'three', 'most', 'import...</td>\n",
       "      <td>['sāmkhya', 'is', 'a', 'dualist', 'philosophic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>['who', 'was', 'costume', 'designer', 'for', '...</td>\n",
       "      <td>['mollo', 'was', 'surprised', 'by', 'the', 'su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>['who', 'developed', 'the', 'first', 'thermonu...</td>\n",
       "      <td>['in', 'the', 'end', ',', 'president', 'truman...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>['what', 'is', 'the', 'population', 'of', 'mah...</td>\n",
       "      <td>['the', 'previous', 'mayor', ',', 'bill', 'laf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                question_text_tokenized  \\\n",
       "0     ['when', 'was', 'quantum', 'field', 'theory', ...   \n",
       "1     ['who', 'was', 'the', 'first', 'nobel', 'prize...   \n",
       "2     ['when', 'is', 'the', 'dialectical', 'method',...   \n",
       "3                    ['who', 'invented', 'hangul', '?']   \n",
       "4            ['what', 'do', 'grasshoppers', 'eat', '?']   \n",
       "...                                                 ...   \n",
       "7384  ['what', 'was', 'neil', 'brooks', \"'\", 'fastes...   \n",
       "7385  ['who', 'are', 'the', 'three', 'most', 'import...   \n",
       "7386  ['who', 'was', 'costume', 'designer', 'for', '...   \n",
       "7387  ['who', 'developed', 'the', 'first', 'thermonu...   \n",
       "7388  ['what', 'is', 'the', 'population', 'of', 'mah...   \n",
       "\n",
       "                           document_plaintext_tokenized  labels  \n",
       "0     ['quantum', 'field', 'theory', 'naturally', 'b...       1  \n",
       "1     ['the', 'nobel', 'prize', 'in', 'literature', ...       1  \n",
       "2     ['dialectic', 'or', 'dialectics', '(', 'greek'...       1  \n",
       "3     ['hangul', 'was', 'personally', 'created', 'an...       1  \n",
       "4     ['grasshoppers', 'are', 'plant-eaters', ',', '...       1  \n",
       "...                                                 ...     ...  \n",
       "7384  ['the', 'medley', 'relay', 'was', 'scheduled',...       0  \n",
       "7385  ['sāmkhya', 'is', 'a', 'dualist', 'philosophic...       0  \n",
       "7386  ['mollo', 'was', 'surprised', 'by', 'the', 'su...       0  \n",
       "7387  ['in', 'the', 'end', ',', 'president', 'truman...       0  \n",
       "7388  ['the', 'previous', 'mayor', ',', 'bill', 'laf...       0  \n",
       "\n",
       "[7389 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_df = dft_eng[[\"question_text_tokenized\", \"document_plaintext_tokenized\", \"labels\"]]\n",
    "\n",
    "bag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdoc_data = [eval(x) for x in dft_eng[\"document_plaintext_tokenized\"]]\n",
    "vdoc_data = [eval(x) for x in dfv_eng[\"document_plaintext_tokenized\"]]\n",
    "\n",
    "tque_data = [eval(x) for x in dft_eng[\"question_text_tokenized\"]]\n",
    "vque_data = [eval(x) for x in dfv_eng[\"question_text_tokenized\"]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2371"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_tdoc = np.array([item for sublist in tdoc_data for item in sublist])\n",
    "flat_vdoc = np.array([item for sublist in vdoc_data for item in sublist])\n",
    "\n",
    "flat_tque = np.array([item for sublist in tque_data for item in sublist])\n",
    "flat_vque = np.array([item for sublist in vque_data for item in sublist])\n",
    "\n",
    "flat_t = np.concatenate((flat_tdoc, flat_tque))\n",
    "flat_v = np.concatenate((flat_vdoc, flat_vque))\n",
    "\n",
    "countt = Counter(flat_t)\n",
    "countv = Counter(flat_v)\n",
    "\n",
    "countt[\"what\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Functions to create BoW vectors\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = np.zeros(len(countt))\n",
    "    for word in sentence:\n",
    "        vec[countt[word]] += 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "print(make_bow_vector(eval(dft_eng[\"document_plaintext_tokenized\"][0]), countt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7389, 65320)\n",
      "(7389, 65320)\n",
      "(990, 65320)\n",
      "(990, 65320)\n"
     ]
    }
   ],
   "source": [
    "X_train_doc = np.array([make_bow_vector(eval(x), countt) for x in dft_eng[\"document_plaintext_tokenized\"]])\n",
    "X_train_que = np.array([make_bow_vector(eval(x), countt) for x in dft_eng[\"question_text_tokenized\"]])\n",
    "\n",
    "X_val_doc = np.array([make_bow_vector(eval(x), countv) for x in dfv_eng[\"document_plaintext_tokenized\"]])\n",
    "X_val_que = np.array([make_bow_vector(eval(x), countv) for x in dfv_eng[\"question_text_tokenized\"]])\n",
    "\n",
    "y_train = dft_eng.labels.values\n",
    "y_val = dfv_eng.labels.values\n",
    "\n",
    "\n",
    "print(X_train_doc.shape)\n",
    "print(X_train_que.shape)\n",
    "print(X_val_doc.shape)\n",
    "print(X_val_que.shape)\n",
    "# print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7343434343434343"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty='l2', random_state=1, solver='liblinear').fit(X_train, y_train)\n",
    "pred = clf.predict(X_val)\n",
    "accuracy_score(pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "c91e34972f49fdaaf3e0b33bc15e67976f947d1e30f99409063fd9e7b2d4183a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
